import os
import operator
import asyncio  # [V3 ä¼˜åŒ–] å¼•å…¥ asyncio å®ç°å¹¶å‘
from typing import Annotated, List, TypedDict, Union

# å¼•å…¥ LangChain å’Œ LangGraph ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
load_dotenv()

# ==============================================================================
# 1. é…ç½® API Key (ä¿æŒä¸ V1 ä¸€è‡´)
# ==============================================================================
api_key = os.environ.get("ARK_API_KEY") or os.environ.get("DOUBAO_API_KEY")
endpoint_id = os.environ.get("DOUBAO_ENDPOINT_ID")
tavily_key = os.environ.get("TAVILY_API_KEY")
offline = os.environ.get("DR_OFFLINE") == "1"
timeout_env = os.environ.get("DR_TIMEOUT")
DEFAULT_TIMEOUT_S = int(timeout_env) if timeout_env and timeout_env.isdigit() else 30

if not api_key and not endpoint_id:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°è±†åŒ… API Key/Endpointï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ LLMã€‚")
if not tavily_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚")


# ==============================================================================
# 2. å®šä¹‰çŠ¶æ€ (State) - V4 å‡çº§ç‰ˆ
# ==============================================================================
class ResearchState(TypedDict):
    topic: str  # åŸå§‹ç ”ç©¶ä¸»é¢˜
    topic_category: str  # [V4 æ–°å¢] ä¸»é¢˜ç±»å‹ (e.g., ç»æµåˆ†æ, æŠ€æœ¯ç»¼è¿°)
    current_queries: List[str]  # å½“å‰è¿™ä¸€è½®éœ€è¦æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢
    all_findings: List[str]  # ç´¯ç§¯æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ (åŒ…å«æ‘˜è¦å’Œå¼•ç”¨URL)
    loop_count: int  # å½“å‰è¿­ä»£æ¬¡æ•° (é˜²æ­¢æ­»å¾ªç¯)
    missing_info: str  # è¯„ä¼°é˜¶æ®µå‘ç°çš„ç¼ºå¤±ä¿¡æ¯ (ç”¨äºæŒ‡å¯¼ä¸‹ä¸€è½®)
    report_outline: str  # æŠ¥å‘Šçš„ç»“æ„å¤§çº² (ç°åœ¨æ˜¯åŠ¨æ€ç”Ÿæˆ)
    final_report: str  # æœ€ç»ˆæŠ¥å‘Š


# ==============================================================================
# 3. åˆå§‹åŒ–æ¨¡å‹å’Œå·¥å…·
# ==============================================================================
# è±†åŒ…æ¨¡å‹ (æ‰€æœ‰èŠ‚ç‚¹éƒ½ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼Œå› æ­¤æ‰€æœ‰èŠ‚ç‚¹å‡½æ•°éƒ½æ”¹ä¸º async)
class _LLMResponse:
    def __init__(self, content: str):
        self.content = content

class _DummyLLM:
    async def ainvoke(self, messages):
        sys = messages[0].content if messages else ""
        human = messages[-1].content if messages else ""
        if "æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢" in sys or "æŸ¥è¯¢åˆ—è¡¨" in sys:
            topic = human.split("ä¸»é¢˜:")[-1].strip() if "ä¸»é¢˜:" in human else "ä¸»é¢˜"
            return _LLMResponse(f"{topic} å®šä¹‰\n{topic} ç°çŠ¶\n{topic} è¶‹åŠ¿")
        if "å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€" in sys:
            return _LLMResponse("æŠ€æœ¯ç»¼è¿°")
        if "æå–å…³é”®äº‹å®" in sys:
            return _LLMResponse("è¦ç‚¹1ã€æ¥æº 1ã€‘\nè¦ç‚¹2ã€æ¥æº 2ã€‘\n---\nå¼•ç”¨: {'[1]': 'https://example.com/1', '[2]': 'https://example.com/2'}")
        if "è‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆ" in sys:
            return _LLMResponse("SUFFICIENT")
        if "é«˜çº§æŠ¥å‘Šç»“æ„å¸ˆ" in sys:
            return _LLMResponse("## èƒŒæ™¯\n## ç°çŠ¶\n## ç«äº‰æ ¼å±€\n## è¶‹åŠ¿")
        if "ä¸“ä¸šåˆ†æå¸ˆ" in sys:
            return _LLMResponse("# æŠ¥å‘Š\n\n## èƒŒæ™¯\nå†…å®¹\n\n## ç°çŠ¶\nå†…å®¹\n\n## ç«äº‰æ ¼å±€\nå†…å®¹\n\n## è¶‹åŠ¿\nå†…å®¹")
        return _LLMResponse("ç¤ºä¾‹è¾“å‡º")

class _DummySearch:
    def __init__(self, max_results: int = 3):
        self.max_results = max_results
    async def ainvoke(self, query: str):
        return [{"url": f"https://example.com/{i}", "content": f"ä¸{query}ç›¸å…³çš„ç¤ºä¾‹å†…å®¹ {i}"} for i in range(1, self.max_results + 1)]

if api_key and not offline:
    model_id = endpoint_id or os.environ.get("DOUBAO_MODEL") or "doubao-seed-1-6-251015"
    llm = ChatOpenAI(
        model=model_id,
        api_key=api_key,
        base_url="https://ark.cn-beijing.volces.com/api/v3",
        temperature=0.1,
    )
else:
    llm = _DummyLLM()

search_tool = TavilySearchResults(max_results=3, tavily_api_key=tavily_key) if tavily_key else _DummySearch(max_results=3)

# å›é€€å®ä¾‹
_FALLBACK_LLM = _DummyLLM()
_FALLBACK_SEARCH = _DummySearch(max_results=3)

# æœ€å¤§è¿­ä»£æ¬¡æ•° (é˜²æ­¢ä¸€ç›´æœä¸ªæ²¡å®Œ)
MAX_LOOPS = 3


# ==============================================================================
# 4. å®šä¹‰èŠ‚ç‚¹é€»è¾‘ (Nodes) - å…¨éƒ¨æ”¹ä¸º async
# ==============================================================================

# ç»Ÿä¸€å¼‚æ­¥è°ƒç”¨å°è£…ï¼Œå…¼å®¹ invoke/ainvoke ä¸¤ç§å®ç°
async def _llm_call(messages, timeout_s: int = DEFAULT_TIMEOUT_S):
    fn = getattr(llm, "ainvoke", None)
    if callable(fn):
        try:
            return await asyncio.wait_for(fn(messages), timeout_s)
        except Exception as e:
            print(f"[è­¦å‘Š] LLM è°ƒç”¨è¶…æ—¶/å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å›é€€ã€‚é”™è¯¯: {e}")
            return await _FALLBACK_LLM.ainvoke(messages)
    invoke_fn = getattr(llm, "invoke", None)
    if callable(invoke_fn):
        try:
            return await asyncio.wait_for(asyncio.to_thread(invoke_fn, messages), timeout_s)
        except Exception as e:
            print(f"[è­¦å‘Š] LLM è°ƒç”¨è¶…æ—¶/å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å›é€€ã€‚é”™è¯¯: {e}")
            return await _FALLBACK_LLM.ainvoke(messages)
    raise RuntimeError("LLM ä¸æ”¯æŒ invoke/ainvoke")

async def _search_call(query: str, timeout_s: int = DEFAULT_TIMEOUT_S):
    fn = getattr(search_tool, "ainvoke", None)
    if callable(fn):
        try: 
            return await asyncio.wait_for(fn(query), timeout_s)
        except Exception as e:
            print(f"[è­¦å‘Š] æœç´¢è°ƒç”¨è¶…æ—¶/å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å›é€€ã€‚é”™è¯¯: {e}")
            return await _FALLBACK_SEARCH.ainvoke(query)
    invoke_fn = getattr(search_tool, "invoke", None)
    if callable(invoke_fn):
        try:
            return await asyncio.wait_for(asyncio.to_thread(invoke_fn, query), timeout_s)
        except Exception as e:
            print(f"[è­¦å‘Š] æœç´¢è°ƒç”¨è¶…æ—¶/å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å›é€€ã€‚é”™è¯¯: {e}")
            return await _FALLBACK_SEARCH.ainvoke(query)
    raise RuntimeError("æœç´¢å·¥å…·ä¸æ”¯æŒ invoke/ainvoke")

async def plan_research(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 1ï¼šåˆå§‹è§„åˆ’ä¸åˆ†ç±»ã€‘
    ç”Ÿæˆåˆå§‹æŸ¥è¯¢å¹¶å¯¹ä¸»é¢˜è¿›è¡Œåˆ†ç±»ï¼Œç”¨äºæŒ‡å¯¼åç»­çš„å¤§çº²ç”Ÿæˆã€‚
    """
    print(f"\nğŸš€ [å¯åŠ¨] å¼€å§‹ç ”ç©¶ä¸»é¢˜: {state['topic']}")

    # æ­¥éª¤ A: ç”Ÿæˆåˆå§‹æŸ¥è¯¢
    planning_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„ä¸»é¢˜æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢ã€‚"
        "æŸ¥è¯¢åº”æ¶µç›–åŸºç¡€å®šä¹‰ã€ç°çŠ¶å’Œä¸»è¦äº‰è®®ç‚¹ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    queries_response = await _llm_call([
        SystemMessage(content=planning_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    queries = [line.strip() for line in queries_response.content.split('\n') if line.strip()][:3]

    # æ­¥éª¤ B: ä¸»é¢˜åˆ†ç±» (V4 æ–°å¢)
    categorization_prompt = (
        "æ ¹æ®ç”¨æˆ·çš„ä¸»é¢˜ï¼Œå°†å…¶å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š[æŠ€æœ¯ç»¼è¿°, å¸‚åœºåˆ†æ, ç»æµè¶‹åŠ¿, å†å²äº‹ä»¶, äººç‰©ä¼ è®°, è¡Œä¸šæŠ¥å‘Š, æ¦‚å¿µè§£é‡Š]ã€‚"
        "è¯·åªè¿”å›æœ€åˆé€‚çš„ç±»åˆ«åç§°ï¼Œä¸å¸¦ä»»ä½•è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·ã€‚"
    )
    category_response = await _llm_call([
        SystemMessage(content=categorization_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    topic_category = category_response.content.strip()

    print(f"ğŸ“‹ [è§„åˆ’] ä¸»é¢˜ç±»å‹: {topic_category} | åˆå§‹æŸ¥è¯¢: {queries}")

    # åˆå§‹åŒ–çŠ¶æ€
    return {
        "current_queries": queries,
        "topic_category": topic_category,
        "all_findings": [],
        "loop_count": 0,
        "missing_info": ""
    }


async def execute_search(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 2ï¼šæ‰§è¡Œæœç´¢ã€‘ - V4: ç¡®ä¿æ‘˜è¦ä¸­åµŒå…¥äº†æ¥æºURLã€‚
    """
    loop_idx = state["loop_count"] + 1
    queries = state["current_queries"]
    print(f"\nğŸ” [ç¬¬ {loop_idx} è½®æœç´¢] æ­£åœ¨å¹¶å‘æ‰§è¡Œ {len(queries)} ä¸ªæŸ¥è¯¢...")

    async def process_query(query):
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ªæŸ¥è¯¢å’Œæ€»ç»“çš„å­ä»»åŠ¡"""
        try:
            # 1. å¼‚æ­¥æœç´¢
            search_results = await _search_call(query)

            # å‡†å¤‡ä¸Šä¸‹æ–‡å’Œå¼•ç”¨æ˜ å°„ï¼ˆé²æ£’è§£æï¼‰
            context_blocks = []
            citation_map = {}
            items = search_results if isinstance(search_results, list) else [search_results]
            for i, res in enumerate(items):
                url = None
                content = None
                if isinstance(res, dict):
                    url = res.get('url') or res.get('source') or res.get('link')
                    content = res.get('content') or res.get('text') or res.get('summary')
                else:
                    url = getattr(res, 'url', None) or getattr(res, 'source', None) or getattr(res, 'link', None)
                    content = getattr(res, 'content', None) or getattr(res, 'page_content', None) or getattr(res, 'text', None)

                if not content:
                    content = str(res)

                if url:
                    citation_map[f"[{i + 1}]"] = url
                context_blocks.append(f"ã€æ¥æº {i + 1}ã€‘: {content}")

            context = "\n".join(context_blocks)

            # 2. å¼‚æ­¥æ€»ç»“ (Information Extraction)
            summary_prompt = (
                f"é’ˆå¯¹æŸ¥è¯¢ '{query}'ï¼Œä»ä»¥ä¸‹ã€æ¥æºã€‘ä¸­æå–å…³é”®äº‹å®ã€æ•°æ®å’Œè§‚ç‚¹ã€‚"
                "åœ¨æ‘˜è¦ä¸­ï¼ŒåŠ¡å¿…ä½¿ç”¨æ ¼å¼ã€æ¥æº Xã€‘å¼•ç”¨ä½ ä½¿ç”¨çš„ä»»ä½•ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š'äººå·¥æ™ºèƒ½æŠ•èµ„åœ¨2023å¹´å¢é•¿äº†30%ã€æ¥æº 2ã€‘'ã€‚"
                "å¿½ç•¥æ— å…³ä¿¡æ¯ã€‚ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ï¼Œå¹¶åˆ—å‡ºå®Œæ•´çš„å¼•ç”¨æ˜ å°„ã€‚"
                f"æœ€ç»ˆè¿”å›æ ¼å¼ï¼š\n---\næ‘˜è¦å†…å®¹\n---\nå¼•ç”¨: {citation_map}"
            )

            summary_response = await _llm_call([
                SystemMessage(content=summary_prompt),
                HumanMessage(content=context)
            ])

            # æå–æ‘˜è¦å’Œå¼•ç”¨ï¼Œå¹¶å°†å…¶åˆå¹¶æˆä¸€ä¸ª V4 æ ¼å¼çš„å‘ç°å—
            # ç¤ºä¾‹ V4 å‘ç°å—ï¼š ### æŸ¥è¯¢ X å‘ç°\n æ‘˜è¦å†…å®¹ \n\n --- å¼•ç”¨æ˜ å°„: {...}
            return f"### å…³äº '{query}' çš„å‘ç° (ç¬¬ {loop_idx} è½®):\n{summary_response.content}"

        except Exception as e:
            print(f"  âŒ æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
            return None

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
    tasks = [process_query(query) for query in queries]
    results = await asyncio.gather(*tasks)

    new_findings = [r for r in results if r]
    total_findings = state["all_findings"] + new_findings

    return {"all_findings": total_findings, "loop_count": loop_idx}


async def evaluate_findings(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 3ï¼šè¯„ä¼°ä¸åæ€ã€‘
    æŸ¥çœ‹å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå†™æŠ¥å‘Šã€‚
    """
    print("\nğŸ¤” [è¯„ä¼°] æ­£åœ¨æ£€æŸ¥èµ„æ–™å®Œæ•´æ€§...")

    topic = state["topic"]
    findings_text = "\n\n".join(state["all_findings"])
    loop_count = state["loop_count"]

    if loop_count >= MAX_LOOPS:
        print("ğŸ›‘ [è¯„ä¼°] å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢æœç´¢ã€‚")
        return {"missing_info": "sufficient"}

        # è®© LLM è¯„ä¼°
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªè‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆã€‚"
        "è¯·é˜…è¯»ç›®å‰æ”¶é›†åˆ°çš„ç¬”è®°ï¼Œåˆ¤æ–­æ˜¯å¦è¶³ä»¥æ’°å†™å…³äºè¯¥ä¸»é¢˜çš„æ·±åº¦æŠ¥å‘Šã€‚"
        "å¦‚æœèµ„æ–™å……è¶³ï¼Œè¯·åªå›å¤ 'SUFFICIENT'ã€‚"
        "å¦‚æœèµ„æ–™ç¼ºå¤±ï¼ˆä¾‹å¦‚ç¼ºå°‘å…·ä½“æ•°æ®ã€åé¢è§‚ç‚¹ã€æœ€æ–°è¿›å±•ï¼‰ï¼Œè¯·å›å¤ 'MISSING: <ç¼ºå¤±å†…å®¹çš„æè¿°>'ã€‚"
        "ä¸è¦å®¢æ°”ï¼Œå¦‚æœä¿¡æ¯å¤ªæµ…æ˜¾ï¼Œå¿…é¡»è¦æ±‚ç»§ç»­æ·±æŒ–ã€‚"
    )

    response = await _llm_call([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç ”ç©¶ä¸»é¢˜: {topic}\n\nç›®å‰ç¬”è®°:\n{findings_text}")
    ])

    if "SUFFICIENT" in response.content.upper():
        print("âœ… [è¯„ä¼°] èµ„æ–™å·²å……è¶³ï¼")
        return {"missing_info": "sufficient"}
    else:
        print(f"âš ï¸ [è¯„ä¼°] å‘ç°ç¼ºå£: {response.content}")
        return {"missing_info": response.content}


async def generate_new_queries(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 4ï¼šç”Ÿæˆè¡¥å……æŸ¥è¯¢ã€‘
    å¦‚æœ evaluate è®¤ä¸ºä¿¡æ¯ç¼ºå¤±ï¼Œè¿™é‡Œè´Ÿè´£ç”Ÿæˆé’ˆå¯¹æ€§çš„æ–°æŸ¥è¯¢ã€‚
    """
    missing_info = state["missing_info"]
    print("\nğŸ”„ [è¿­ä»£] æ­£åœ¨ç”Ÿæˆè¡¥å……æŸ¥è¯¢ä»¥å¡«è¡¥ç¼ºå£...")

    system_prompt = (
        "æ ¹æ®ç¼ºå¤±çš„ä¿¡æ¯æè¿°ï¼Œç”Ÿæˆ 2 ä¸ªå…·ä½“çš„æœç´¢å¼•æ“æŸ¥è¯¢è¯­å¥æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    response = await _llm_call([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç¼ºå¤±ä¿¡æ¯: {missing_info}")
    ])

    new_queries = [line.strip() for line in response.content.split('\n') if line.strip()][:2]
    print(f"ğŸ†• [è¡¥å……æŸ¥è¯¢] {new_queries}")

    return {"current_queries": new_queries}


async def outline_report(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 5ï¼šåŠ¨æ€ç”ŸæˆæŠ¥å‘Šå¤§çº²ã€‘ - V4: æ ¹æ®ä¸»é¢˜ç±»åˆ«å’Œåˆæ­¥å‘ç°ç”Ÿæˆå¤§çº²ã€‚
    """
    print("\nğŸ“ [ç»“æ„] æ­£åœ¨ç”ŸæˆåŠ¨æ€æŠ¥å‘Šå¤§çº²...")

    topic = state["topic"]
    category = state["topic_category"]
    findings_preview = "\n\n".join(state["all_findings"])[:2000]  # ä¼ é€’éƒ¨åˆ†å‘ç°ä½œä¸ºä¸Šä¸‹æ–‡

    system_prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªé«˜çº§æŠ¥å‘Šç»“æ„å¸ˆã€‚ä¸»é¢˜ç±»åˆ«æ˜¯ '{category}'ã€‚"
        "è¯·æ ¹æ®è¿™ä¸ªç±»åˆ«å’Œä»¥ä¸‹åˆæ­¥ç ”ç©¶ç¬”è®°ï¼Œç”Ÿæˆä¸€ä»½æœ€ä¸“ä¸šã€æœ€ç›¸å…³çš„æŠ¥å‘Šå¤§çº²ã€‚"
        "ä¾‹å¦‚ï¼Œå¦‚æœæ˜¯'å¸‚åœºåˆ†æ'ï¼Œå¤§çº²åº”åŒ…æ‹¬'å¸‚åœºè§„æ¨¡'ã€'ç«äº‰æ ¼å±€'ã€'SWOT'ï¼›å¦‚æœæ˜¯'å†å²äº‹ä»¶'ï¼Œåˆ™åº”åŒ…å«'èƒŒæ™¯'ã€'è¿‡ç¨‹'ã€'å½±å“'ã€‚"
        "å¤§çº²åº”è‡³å°‘åŒ…å« 4 ä¸ªä¸»è¦ç« èŠ‚ï¼ˆMarkdown äºŒçº§æ ‡é¢˜ ##ï¼‰ï¼Œå¹¶ç›´æ¥è¿”å› Markdown æ ¼å¼çš„å¤§çº²ã€‚"
    )

    user_prompt = f"ç ”ç©¶ä¸»é¢˜: {topic}\nä¸»é¢˜ç±»åˆ«: {category}\n\nåˆæ­¥ç ”ç©¶ç¬”è®°é¢„è§ˆ:\n{findings_preview}"

    response = await _llm_call([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    print(f"âœ… [ç»“æ„] å¤§çº²å·²ç”Ÿæˆï¼ŒåŸºäºç±»åˆ«: {category}ã€‚")
    return {"report_outline": response.content}


async def write_report(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 6ï¼šæ’°å†™æŠ¥å‘Šã€‘ - V4: å¿…é¡»æ ¹æ®åµŒå…¥çš„å¼•ç”¨ä¿¡æ¯ï¼Œåœ¨æŠ¥å‘Šæœ«å°¾åˆ—å‡ºå‚è€ƒèµ„æ–™ã€‚
    """
    print("\nâœï¸ [å†™ä½œ] æ­£åœ¨æ•´åˆæ‰€æœ‰èµ„æ–™æ’°å†™æŠ¥å‘Š...")

    context = "\n\n".join(state["all_findings"])
    outline = state["report_outline"]

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„ç ”ç©¶ç¬”è®°å’Œä»¥ä¸‹å¤§çº²ï¼Œå†™å‡ºä¸€ä»½ç»“æ„ä¸¥è°¨ã€æ•°æ®è¯¦å®çš„æ·±åº¦æŠ¥å‘Š(Markdownæ ¼å¼)ã€‚"
        "ä¸¥æ ¼éµå¾ªå¤§çº²ç»“æ„ã€‚"
        "å†™ä½œæ—¶ï¼Œå¿…é¡»å‚è€ƒç¬”è®°ä¸­çš„ã€æ¥æº Xã€‘æ ‡è®°ï¼Œå¹¶å°†è¿™äº›å¼•ç”¨ä¿¡æ¯åœ¨æŠ¥å‘Šæœ«å°¾çš„'å‚è€ƒèµ„æ–™'éƒ¨åˆ†å®Œæ•´åˆ—å‡ºï¼Œä½¿ç”¨æ ‡å‡†URLæ ¼å¼ã€‚"
    )

    user_prompt = f"ä¸»é¢˜: {state['topic']}\n\nç»“æ„å¤§çº²:\n{outline}\n\nç ”ç©¶ç¬”è®°:\n{context}"

    response = await _llm_call([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    return {"final_report": response.content}


# ==============================================================================
# 5. æ„å»ºå›¾é€»è¾‘ (Routing Logic)
# ==============================================================================

def should_continue(state: ResearchState):
    """
    æ¡ä»¶è¾¹é€»è¾‘ï¼šå†³å®šæ˜¯å›å»æ¥ç€æœï¼Œè¿˜æ˜¯å»å†™æŠ¥å‘Š
    """
    missing = state.get("missing_info", "")
    if missing == "sufficient" or state["loop_count"] >= MAX_LOOPS:
        return "to_outline"
    else:
        return "to_generator"


# åˆå§‹åŒ–å›¾
workflow = StateGraph(ResearchState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("planner", plan_research)
workflow.add_node("researcher", execute_search)
workflow.add_node("evaluator", evaluate_findings)
workflow.add_node("query_generator", generate_new_queries)
workflow.add_node("outline_planner", outline_report)
workflow.add_node("writer", write_report)

# æ„å»ºæµç¨‹
workflow.set_entry_point("planner")
workflow.add_edge("planner", "researcher")
workflow.add_edge("researcher", "evaluator")

# è¯„ä¼° -> æ¡ä»¶åˆ¤æ–­
workflow.add_conditional_edges(
    "evaluator",
    should_continue,
    {
        "to_generator": "query_generator",
        "to_outline": "outline_planner"
    }
)

# è¿­ä»£å¾ªç¯
workflow.add_edge("query_generator", "researcher")

# ç»“æ„åŒ–å†™ä½œ
workflow.add_edge("outline_planner", "writer")
workflow.add_edge("writer", END)

app = workflow.compile()


# ==============================================================================
# 6. è¿è¡Œå…¥å£
# ==============================================================================

async def run_agent():
    print("=== Deep Research Agent V4 (åŠ¨æ€ç»“æ„ & å¼•ç”¨æº¯æº) ===")
    topic = input("è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜: ")
    if not topic: topic = "é‡å­è®¡ç®—æœºåœ¨2024å¹´çš„æœ€æ–°çªç ´"

    initial_state = {"topic": topic}

    final_state = await app.ainvoke(initial_state)

    print("\n" + "=" * 50)
    print("æœ€ç»ˆæŠ¥å‘Š:")
    print(final_state["final_report"])

    # ä¿å­˜æ–‡ä»¶
    with open("deep_research_v4.md", "w", encoding="utf-8") as f:
        f.write(final_state["final_report"])
    print("\n[ç³»ç»Ÿ] æŠ¥å‘Šå·²ä¿å­˜è‡³ deep_research_v4.md")


if __name__ == "__main__":
    asyncio.run(run_agent())

async def run_agent_once(topic: str):
    initial_state = {"topic": topic}
    final_state = await app.ainvoke(initial_state)
    return final_state
