import os
import operator
import asyncio
import re
from typing import Annotated, List, TypedDict, Union

# å¼•å…¥ LangChain å’Œ LangGraph ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
load_dotenv()

# ==============================================================================
# 1. é…ç½® API Key
# ==============================================================================
api_key = os.environ.get("ARK_API_KEY") or os.environ.get("DOUBAO_API_KEY")
tavily_key = os.environ.get("TAVILY_API_KEY")

if not api_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°è±†åŒ… API Key/Endpointï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ LLMã€‚")
if not tavily_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚")


# ==============================================================================
# 2. å®šä¹‰çŠ¶æ€ (State) - V7 å‡çº§ç‰ˆ (æ”¯æŒç« èŠ‚è¿­ä»£å’Œè®°å¿†å‹ç¼©)
# ==============================================================================
class ResearchState(TypedDict):
    topic: str  # åŸå§‹ç ”ç©¶ä¸»é¢˜
    topic_category: str  # ä¸»é¢˜ç±»å‹
    current_queries: List[str]  # å½“å‰è¿™ä¸€è½®éœ€è¦æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢
    all_findings: List[str]  # ç´¯ç§¯æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ (è¯¦ç»†æ‘˜è¦æˆ–å‹ç¼©åçš„é•¿æœŸè®°å¿†)

    # V6/V7 å†™ä½œè¿­ä»£çŠ¶æ€
    report_outline: str  # æŠ¥å‘Šçš„å®Œæ•´ç»“æ„å¤§çº² (Markdown string)
    remaining_chapters: List[str]  # å¾…å†™ä½œçš„ç« èŠ‚æ ‡é¢˜åˆ—è¡¨
    current_chapter: str  # æ­£åœ¨å¤„ç†çš„ç« èŠ‚æ ‡é¢˜
    refined_context: str  # ç»è¿‡ç²¾ç‚¼å’Œç­›é€‰çš„**å½“å‰ç« èŠ‚**å†™ä½œä¸Šä¸‹æ–‡
    report_sections: List[str]  # å·²å®Œæˆçš„ç« èŠ‚å†…å®¹ï¼ˆMarkdownï¼‰

    loop_count: int  # å½“å‰è¿­ä»£æ¬¡æ•° (é˜²æ­¢æ­»å¾ªç¯)
    missing_info: str  # è¯„ä¼°é˜¶æ®µå‘ç°çš„ç¼ºå¤±ä¿¡æ¯ (ç”¨äºæŒ‡å¯¼ä¸‹ä¸€è½®)
    final_report: str  # æœ€ç»ˆæŠ¥å‘Š


# ==============================================================================
# 3. åˆå§‹åŒ–æ¨¡å‹å’Œå·¥å…·
# ==============================================================================
# è±†åŒ…æ¨¡å‹ (æ‰€æœ‰èŠ‚ç‚¹éƒ½ä½¿ç”¨å¼‚æ­¥è°ƒç”¨)
class _LLMResponse:
    def __init__(self, content: str):
        self.content = content

class _DummyLLM:
    async def ainvoke(self, messages):
        sys = messages[0].content if messages else ""
        human = messages[-1].content if messages else ""
        if "æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢" in sys or "æŸ¥è¯¢åˆ—è¡¨" in sys:
            topic = human.split("ä¸»é¢˜:")[-1].strip() if "ä¸»é¢˜:" in human else "ä¸»é¢˜"
            return _LLMResponse(f"{topic} å®šä¹‰\n{topic} ç°çŠ¶\n{topic} è¶‹åŠ¿")
        if "å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€" in sys:
            return _LLMResponse("æŠ€æœ¯ç»¼è¿°")
        if "æå–å…³é”®äº‹å®" in sys:
            return _LLMResponse("è¦ç‚¹1ã€æ¥æº 1ã€‘\nè¦ç‚¹2ã€æ¥æº 2ã€‘\n---\nå¼•ç”¨: {'[1]': 'https://example.com/1', '[2]': 'https://example.com/2'}")
        if "è‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆ" in sys:
            return _LLMResponse("SUFFICIENT")
        if "é«˜çº§æŠ¥å‘Šç»“æ„å¸ˆ" in sys:
            return _LLMResponse("## èƒŒæ™¯\n## ç°çŠ¶\n## ç«äº‰æ ¼å±€\n## è¶‹åŠ¿")
        if "ä¸Šä¸‹æ–‡å‹ç¼©ä¸“å®¶" in sys and "ç« èŠ‚æ ‡é¢˜" in human:
            return _LLMResponse("ä¸ç« èŠ‚ç›´æ¥ç›¸å…³çš„è¦ç‚¹ã€æ¥æº 1ã€‘")
        if "ä¸“ä¸šæŠ¥å‘Šæ’°ç¨¿äºº" in sys:
            return _LLMResponse("æ®µè½å†…å®¹ï¼ŒåŒ…å«å¼•ç”¨ã€æ¥æº 1ã€‘")
        return _LLMResponse("ç¤ºä¾‹è¾“å‡º")

class _DummySearch:
    def __init__(self, max_results: int = 3):
        self.max_results = max_results
    async def ainvoke(self, query: str):
        return [{"url": f"https://example.com/{i}", "content": f"ä¸{query}ç›¸å…³çš„ç¤ºä¾‹å†…å®¹ {i}"} for i in range(1, self.max_results + 1)]

llm = ChatOpenAI(
    model="doubao-seed-1-6-251015",
    api_key=api_key,
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    temperature=0.1,
) if api_key else _DummyLLM()

search_tool = TavilySearchResults(max_results=3) if tavily_key else _DummySearch(max_results=3)

# æœ€å¤§è¿­ä»£æ¬¡æ•°
MAX_LOOPS = 3
# è®°å¿†å‹ç¼©é˜ˆå€¼ (all_findings è¶…è¿‡æ­¤æ•°é‡åè§¦å‘å‹ç¼©)
COMPRESSION_THRESHOLD = 6


# ==============================================================================
# 4. å®šä¹‰èŠ‚ç‚¹é€»è¾‘ (Nodes) - å…¨éƒ¨æ”¹ä¸º async
# ==============================================================================

async def plan_research(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 1ï¼šåˆå§‹è§„åˆ’ä¸åˆ†ç±»ã€‘
    ç”Ÿæˆåˆå§‹æŸ¥è¯¢å¹¶å¯¹ä¸»é¢˜è¿›è¡Œåˆ†ç±»ï¼Œç”¨äºæŒ‡å¯¼åç»­çš„å¤§çº²ç”Ÿæˆã€‚
    """
    print(f"\nğŸš€ [å¯åŠ¨] å¼€å§‹ç ”ç©¶ä¸»é¢˜: {state['topic']}")

    # æ­¥éª¤ A: ç”Ÿæˆåˆå§‹æŸ¥è¯¢
    planning_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„ä¸»é¢˜æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢ã€‚"
        "æŸ¥è¯¢åº”æ¶µç›–åŸºç¡€å®šä¹‰ã€ç°çŠ¶å’Œä¸»è¦äº‰è®®ç‚¹ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    queries_response = await llm.ainvoke([
        SystemMessage(content=planning_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    queries = [line.strip() for line in queries_response.content.split('\n') if line.strip()][:3]

    # æ­¥éª¤ B: ä¸»é¢˜åˆ†ç±»
    categorization_prompt = (
        "æ ¹æ®ç”¨æˆ·çš„ä¸»é¢˜ï¼Œå°†å…¶å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š[æŠ€æœ¯ç»¼è¿°, å¸‚åœºåˆ†æ, ç»æµè¶‹åŠ¿, å†å²äº‹ä»¶, äººç‰©ä¼ è®°, è¡Œä¸šæŠ¥å‘Š, æ¦‚å¿µè§£é‡Š]ã€‚"
        "è¯·åªè¿”å›æœ€åˆé€‚çš„ç±»åˆ«åç§°ï¼Œä¸å¸¦ä»»ä½•è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·ã€‚"
    )
    category_response = await llm.ainvoke([
        SystemMessage(content=categorization_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    topic_category = category_response.content.strip()

    print(f"ğŸ“‹ [è§„åˆ’] ä¸»é¢˜ç±»å‹: {topic_category} | åˆå§‹æŸ¥è¯¢: {queries}")

    # åˆå§‹åŒ–çŠ¶æ€
    return {
        "current_queries": queries,
        "topic_category": topic_category,
        "all_findings": [],
        "loop_count": 0,
        "missing_info": "",
        "report_sections": []
    }


async def execute_search(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 2ï¼šæ‰§è¡Œæœç´¢ã€‘ - å¹¶å‘æœç´¢ï¼Œç¡®ä¿æ‘˜è¦ä¸­åµŒå…¥äº†æ¥æºURLã€‚
    """
    loop_idx = state["loop_count"] + 1
    queries = state["current_queries"]
    print(f"\nğŸ” [ç¬¬ {loop_idx} è½®æœç´¢] æ­£åœ¨å¹¶å‘æ‰§è¡Œ {len(queries)} ä¸ªæŸ¥è¯¢...")

    # ç®€å•å®ç°æŒ‡æ•°é€€é¿ (Exponential Backoff) æœºåˆ¶
    async def api_call_with_retry(llm_input, max_retries=3):
        for attempt in range(max_retries):
            try:
                return await llm.ainvoke(llm_input)
            except Exception as e:
                if attempt < max_retries - 1:
                    delay = 2 ** attempt  # 1s, 2s, 4s...
                    print(f"   âš ï¸ API è°ƒç”¨å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {delay}s...")
                    await asyncio.sleep(delay)
                else:
                    raise e
        return None

    async def process_query(query):
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ªæŸ¥è¯¢å’Œæ€»ç»“çš„å­ä»»åŠ¡"""
        try:
            # 1. å¼‚æ­¥æœç´¢
            search_results = await search_tool.ainvoke(query)

            # å‡†å¤‡ä¸Šä¸‹æ–‡å’Œå¼•ç”¨æ˜ å°„
            context_blocks = []
            citation_map = {}
            for i, res in enumerate(search_results):
                # ä¸ºæ¯ä¸ªæ¥æºåˆ†é…ä¸€ä¸ªä¸´æ—¶ç¼–å·ç”¨äºæ‘˜è¦å¼•ç”¨
                citation_map[f"[{i + 1}]"] = res['url']
                context_blocks.append(f"ã€æ¥æº {i + 1}ã€‘: {res['content']}")

            context = "\n".join(context_blocks)

            # 2. å¼‚æ­¥æ€»ç»“ (Information Extraction) - ä½¿ç”¨é‡è¯•æœºåˆ¶
            summary_prompt = (
                f"é’ˆå¯¹æŸ¥è¯¢ '{query}'ï¼Œä»ä»¥ä¸‹ã€æ¥æºã€‘ä¸­æå–å…³é”®äº‹å®ã€æ•°æ®å’Œè§‚ç‚¹ã€‚"
                "åœ¨æ‘˜è¦ä¸­ï¼ŒåŠ¡å¿…ä½¿ç”¨æ ¼å¼ã€æ¥æº Xã€‘å¼•ç”¨ä½ ä½¿ç”¨çš„ä»»ä½•ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š'äººå·¥æ™ºèƒ½æŠ•èµ„åœ¨2023å¹´å¢é•¿äº†30%ã€æ¥æº 2ã€‘'ã€‚"
                "å¿½ç•¥æ— å…³ä¿¡æ¯ã€‚ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ï¼Œå¹¶åˆ—å‡ºå®Œæ•´çš„å¼•ç”¨æ˜ å°„ã€‚"
                f"æœ€ç»ˆè¿”å›æ ¼å¼ï¼š\n---\næ‘˜è¦å†…å®¹\n---\nå¼•ç”¨: {citation_map}"
            )

            summary_response = await api_call_with_retry([
                SystemMessage(content=summary_prompt),
                HumanMessage(content=context)
            ])

            # æå–æ‘˜è¦å’Œå¼•ç”¨ï¼Œå¹¶å°†å…¶åˆå¹¶æˆä¸€ä¸ª V4 æ ¼å¼çš„å‘ç°å—
            return f"### å…³äº '{query}' çš„å‘ç° (ç¬¬ {loop_idx} è½®):\n{summary_response.content}"

        except Exception as e:
            print(f"  âŒ æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
            return None

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
    tasks = [process_query(query) for query in queries]
    results = await asyncio.gather(*tasks)

    new_findings = [r for r in results if r]
    total_findings = state["all_findings"] + new_findings

    return {"all_findings": total_findings, "loop_count": loop_idx}


async def recursive_summarizer(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 2.5 (V7 æ–°å¢)ï¼šé€’å½’è®°å¿†å‹ç¼©ã€‘
    æ¨¡æ‹Ÿ MemGPT æœºåˆ¶ï¼Œå‘¨æœŸæ€§åœ°å‹ç¼©æ—§çš„ã€è¯¦ç»†çš„å‘ç°ï¼Œä¿æŒä¸Šä¸‹æ–‡ç²¾ç®€ã€‚
    """
    findings = state["all_findings"]

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å‹ç¼©é˜ˆå€¼
    if len(findings) < COMPRESSION_THRESHOLD:
        print("ğŸ’¡ [è®°å¿†] å‘ç°é¡¹ä¸è¶³ï¼Œè·³è¿‡é€’å½’æ‘˜è¦ã€‚")
        return {}

    # 1. è¯†åˆ«éœ€è¦å‹ç¼©çš„â€œæ—§â€è®°å¿† (å‹ç¼©åˆ—è¡¨çš„å‰åŠéƒ¨åˆ†)
    split_point = len(findings) // 2
    old_findings_to_compress = findings[:split_point]
    new_findings_to_keep = findings[split_point:]

    print(f"\nğŸ§  [è®°å¿†å‹ç¼©] å‘ç° {len(findings)} é¡¹ï¼Œæ­£åœ¨å‹ç¼©å‰ {split_point} é¡¹ (æ—§è®°å¿†)...")

    context_to_summarize = "\n\n---\n\n".join(old_findings_to_compress)

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªé€’å½’è®°å¿†å‹ç¼©å¼•æ“ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸‹æ–¹æä¾›çš„æ—§ç ”ç©¶å‘ç°ï¼Œå‹ç¼©æˆä¸€ä¸ª**å•ä¸€ã€é«˜å±‚æ¬¡ã€æµ“ç¼©çš„æ‘˜è¦**ã€‚"
        "ç›®æ ‡æ˜¯ä¿ç•™æ ¸å¿ƒäº‹å®å’Œè¶‹åŠ¿ï¼Œä½†ç§»é™¤ä¸å¿…è¦çš„ç»†èŠ‚ï¼Œä»¥ä¾¿ä¸ºæ–°çš„ç ”ç©¶å‘ç°è…¾å‡ºå†…å­˜ç©ºé—´ã€‚"
        "å‹ç¼©åçš„æ‘˜è¦å¿…é¡»ä»¥ '### é€’å½’é•¿æœŸè®°å¿†æ‘˜è¦:' å¼€å¤´ã€‚"
        "è¯·åŠ¡å¿…åœ¨æ‘˜è¦ä¸­ä¿ç•™æ‰€æœ‰åŸå§‹çš„ã€æ¥æº Xã€‘å¼•ç”¨æ ‡è®°ã€‚"
        "åªè¿”å›æ‘˜è¦å†…å®¹ï¼Œä¸å¸¦å…¶ä»–å¼•å¯¼è¯ã€‚"
    )

    # ä½¿ç”¨é‡è¯•æœºåˆ¶è°ƒç”¨ LLM
    async def api_call_with_retry(llm_input, max_retries=3):
        for attempt in range(max_retries):
            try:
                return await llm.ainvoke(llm_input)
            except Exception as e:
                if attempt < max_retries - 1:
                    delay = 2 ** attempt
                    await asyncio.sleep(delay)
                else:
                    raise e
        return None

    response = await api_call_with_retry([
        SystemMessage(content=system_prompt),
        HumanMessage(content=context_to_summarize)
    ])

    compressed_summary = response.content.strip()

    # 2. æ›´æ–° all_findings: [å‹ç¼©åçš„æ‘˜è¦] + [æ–°çš„/æœªå‹ç¼©çš„å‘ç°]
    new_all_findings = [compressed_summary] + new_findings_to_keep

    print(f"âœ… [è®°å¿†] è®°å¿†å‹ç¼©å®Œæˆã€‚å½“å‰å‘ç°é¡¹æ•°é‡: {len(new_all_findings)}")

    return {"all_findings": new_all_findings}


async def evaluate_findings(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 3ï¼šè¯„ä¼°ä¸åæ€ã€‘
    æŸ¥çœ‹å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå†™æŠ¥å‘Šã€‚
    """
    print("\nğŸ¤” [è¯„ä¼°] æ­£åœ¨æ£€æŸ¥èµ„æ–™å®Œæ•´æ€§...")

    topic = state["topic"]
    findings_text = "\n\n".join(state["all_findings"])
    loop_count = state["loop_count"]

    if loop_count >= MAX_LOOPS:
        print("ğŸ›‘ [è¯„ä¼°] å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢æœç´¢ã€‚")
        return {"missing_info": "sufficient"}

        # è®© LLM è¯„ä¼°
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªè‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆã€‚"
        "è¯·é˜…è¯»ç›®å‰æ”¶é›†åˆ°çš„ç¬”è®°ï¼ˆåŒ…æ‹¬é€’å½’æ‘˜è¦ï¼‰ï¼Œåˆ¤æ–­æ˜¯å¦è¶³ä»¥æ’°å†™å…³äºè¯¥ä¸»é¢˜çš„æ·±åº¦æŠ¥å‘Šã€‚"
        "å¦‚æœèµ„æ–™å……è¶³ï¼Œè¯·åªå›å¤ 'SUFFICIENT'ã€‚"
        "å¦‚æœèµ„æ–™ç¼ºå¤±ï¼ˆä¾‹å¦‚ç¼ºå°‘å…·ä½“æ•°æ®ã€åé¢è§‚ç‚¹ã€æœ€æ–°è¿›å±•ï¼‰ï¼Œè¯·å›å¤ 'MISSING: <ç¼ºå¤±å†…å®¹çš„æè¿°>'ã€‚"
        "ä¸è¦å®¢æ°”ï¼Œå¦‚æœä¿¡æ¯å¤ªæµ…æ˜¾ï¼Œå¿…é¡»è¦æ±‚ç»§ç»­æ·±æŒ–ã€‚"
    )

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç ”ç©¶ä¸»é¢˜: {topic}\n\nç›®å‰ç¬”è®°:\n{findings_text}")
    ])

    if "SUFFICIENT" in response.content.upper():
        print("âœ… [è¯„ä¼°] èµ„æ–™å·²å……è¶³ï¼")
        return {"missing_info": "sufficient"}
    else:
        print(f"âš ï¸ [è¯„ä¼°] å‘ç°ç¼ºå£: {response.content}")
        return {"missing_info": response.content}


async def generate_new_queries(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 4ï¼šç”Ÿæˆè¡¥å……æŸ¥è¯¢ã€‘
    å¦‚æœ evaluate è®¤ä¸ºä¿¡æ¯ç¼ºå¤±ï¼Œè¿™é‡Œè´Ÿè´£ç”Ÿæˆé’ˆå¯¹æ€§çš„æ–°æŸ¥è¯¢ã€‚
    """
    missing_info = state["missing_info"]
    print("\nğŸ”„ [è¿­ä»£] æ­£åœ¨ç”Ÿæˆè¡¥å……æŸ¥è¯¢ä»¥å¡«è¡¥ç¼ºå£...")

    system_prompt = (
        "æ ¹æ®ç¼ºå¤±çš„ä¿¡æ¯æè¿°ï¼Œç”Ÿæˆ 2 ä¸ªå…·ä½“çš„æœç´¢å¼•æ“æŸ¥è¯¢è¯­å¥æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç¼ºå¤±ä¿¡æ¯: {missing_info}")
    ])

    new_queries = [line.strip() for line in response.content.split('\n') if line.strip()][:2]
    print(f"ğŸ†• [è¡¥å……æŸ¥è¯¢] {new_queries}")

    return {"current_queries": new_queries}


async def outline_report(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 5ï¼šåŠ¨æ€ç”ŸæˆæŠ¥å‘Šå¤§çº²ã€‘ - æ ¹æ®ä¸»é¢˜ç±»åˆ«å’Œåˆæ­¥å‘ç°ç”Ÿæˆå¤§çº²ã€‚
    """
    print("\nğŸ“ [ç»“æ„] æ­£åœ¨ç”ŸæˆåŠ¨æ€æŠ¥å‘Šå¤§çº²...")

    topic = state["topic"]
    category = state["topic_category"]
    # ä¼ å…¥å‹ç¼©åçš„å…¨éƒ¨å‘ç°
    findings_preview = "\n\n".join(state["all_findings"])

    system_prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªé«˜çº§æŠ¥å‘Šç»“æ„å¸ˆã€‚ä¸»é¢˜ç±»åˆ«æ˜¯ '{category}'ã€‚"
        "è¯·æ ¹æ®è¿™ä¸ªç±»åˆ«å’Œä»¥ä¸‹åˆæ­¥ç ”ç©¶ç¬”è®°ï¼Œç”Ÿæˆä¸€ä»½æœ€ä¸“ä¸šã€æœ€ç›¸å…³çš„æŠ¥å‘Šå¤§çº²ã€‚"
        "å¤§çº²åº”è‡³å°‘åŒ…å« 4 ä¸ªä¸»è¦ç« èŠ‚ï¼ˆMarkdown äºŒçº§æ ‡é¢˜ ##ï¼‰ï¼Œå¹¶ç›´æ¥è¿”å› Markdown æ ¼å¼çš„å¤§çº²ã€‚"
        "æ³¨æ„ï¼šä¸è¦åœ¨äºŒçº§æ ‡é¢˜ä¸­åŒ…å« 'å¼•è¨€' æˆ– 'ç»“è®º'ï¼Œç•™ç»™åé¢çš„èŠ‚ç‚¹å¤„ç†ã€‚"
    )

    user_prompt = f"ç ”ç©¶ä¸»é¢˜: {topic}\nä¸»é¢˜ç±»åˆ«: {category}\n\nåˆæ­¥ç ”ç©¶ç¬”è®°:\n{findings_preview}"

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    print(f"âœ… [ç»“æ„] å¤§çº²å·²ç”Ÿæˆï¼ŒåŸºäºç±»åˆ«: {category}ã€‚")
    return {"report_outline": response.content}


async def parse_outline_and_prepare_chapters(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 6ï¼šè§£æå¤§çº²å¹¶å‡†å¤‡ç« èŠ‚ã€‘
    å°†å®Œæ•´çš„Markdownå¤§çº²è§£æä¸ºå¾…å†™ä½œçš„ç« èŠ‚åˆ—è¡¨ï¼Œå¹¶è®¾ç½®ç¬¬ä¸€ä¸ªç« èŠ‚ã€‚
    """
    print("\nğŸ“š [å‡†å¤‡] æ­£åœ¨è§£æå¤§çº²å¹¶å‡†å¤‡è¿­ä»£å†™ä½œ...")

    outline = state["report_outline"]

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ‰€æœ‰ Markdown äºŒçº§æ ‡é¢˜
    chapter_titles = re.findall(r'##\s*(.*)', outline)

    # å¢åŠ æ ‡å‡†çš„å¼•è¨€å’Œç»“è®ºä½œä¸ºè¿­ä»£çš„é¦–å°¾ç« èŠ‚
    all_chapters = ["å¼•è¨€"] + chapter_titles + ["ç»“è®º"]

    if not all_chapters:
        return {"remaining_chapters": [], "current_chapter": ""}

    # å¼¹å‡ºç¬¬ä¸€ä¸ªä½œä¸ºå½“å‰ç« èŠ‚
    current_chapter = all_chapters.pop(0)

    print(f"â¡ï¸ [å½“å‰ç« èŠ‚] '{current_chapter}' | å‰©ä½™ {len(all_chapters)} ç« å¾…å†™ã€‚")

    return {
        "remaining_chapters": all_chapters,
        "current_chapter": current_chapter,
    }


async def chapter_context_retriever(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 7: ç« èŠ‚ä¸Šä¸‹æ–‡æ£€ç´¢å™¨ã€‘
    æ ¹æ®å½“å‰ç« èŠ‚æ ‡é¢˜å’Œæ‰€æœ‰èµ„æ–™ï¼Œæç‚¼å‡ºæœ€å…³é”®çš„ä¸Šä¸‹æ–‡ã€‚
    """
    print(f"\nâœ‚ï¸ [æ£€ç´¢] æ­£åœ¨ä¸ºç« èŠ‚ '{state['current_chapter']}' æç‚¼æ ¸å¿ƒä¸Šä¸‹æ–‡...")

    chapter_title = state["current_chapter"]
    all_findings = "\n\n".join(state["all_findings"])

    # æç‚¼æŒ‡ä»¤æ›´åŠ ç²¾ç¡®ï¼Œèšç„¦äºå½“å‰ç« èŠ‚
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å‹ç¼©ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„**ç« èŠ‚æ ‡é¢˜**ï¼Œä»ä¸‹æ–¹æ‰€æœ‰ç ”ç©¶å‘ç°ï¼ˆåŒ…å«è¯¦ç»†å‘ç°å’Œé•¿æœŸè®°å¿†æ‘˜è¦ï¼‰ä¸­ï¼Œ"
        "ä»…æŒ‘é€‰å‡º**ä¸è¯¥ç« èŠ‚ä¸»é¢˜ç›´æ¥ç›¸å…³**çš„äº‹å®ã€æ•°æ®å’Œå¼•ç”¨ä¿¡æ¯ã€‚"
        "è¯·å°†æç‚¼åçš„ä¿¡æ¯ä»¥ç²¾ç®€ã€ç»“æ„åŒ–çš„æ–¹å¼è¿”å›ï¼Œ**åŠ¡å¿…ä¿ç•™æ‰€æœ‰ã€æ¥æº Xã€‘æ ‡è®°**ã€‚"
        "ç›®æ ‡ï¼šå°†ä¸Šä¸‹æ–‡å‹ç¼©åˆ°æœ€ç²¾ç®€ï¼Œåªä¿ç•™æ’°å†™è¯¥ç« èŠ‚æ‰€éœ€çš„æ ¸å¿ƒè®ºæ®ã€‚"
    )

    user_prompt = f"ç« èŠ‚æ ‡é¢˜: {chapter_title}\n\nå…¨éƒ¨åŸå§‹ç ”ç©¶å‘ç°:\n{all_findings}"

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    return {"refined_context": response.content}


async def chapter_writer(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 8ï¼šç« èŠ‚æ’°å†™å™¨ã€‘
    ä½¿ç”¨ç²¾ç‚¼åçš„ä¸Šä¸‹æ–‡ï¼Œåªæ’°å†™å½“å‰ç« èŠ‚çš„å†…å®¹ã€‚
    """
    chapter_title = state["current_chapter"]
    refined_context = state["refined_context"]

    print(f"\nâœï¸ [å†™ä½œ] æ­£åœ¨æ’°å†™ç« èŠ‚: '{chapter_title}'...")

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæŠ¥å‘Šæ’°ç¨¿äººã€‚è¯·æ ¹æ®æä¾›çš„**ç²¾ç‚¼ä¸Šä¸‹æ–‡**ï¼Œæ’°å†™å…³äºä¸»é¢˜çš„**ä¸€ä¸ªç‹¬ç«‹ç« èŠ‚**ã€‚"
        "å¦‚æœç« èŠ‚æ˜¯'å¼•è¨€'æˆ–'ç»“è®º'ï¼Œè¯·ç›¸åº”è°ƒæ•´å†™ä½œé£æ ¼ã€‚"
        "å¦‚æœç« èŠ‚æ˜¯ä¸»ä½“å†…å®¹ï¼Œè¯·å°†æ ‡é¢˜ä½œä¸º Markdown äºŒçº§æ ‡é¢˜ï¼ˆä¾‹å¦‚ï¼š## ç« èŠ‚æ ‡é¢˜ï¼‰å¼€å¤´ï¼Œç„¶åæ’°å†™å†…å®¹ã€‚"
        "å¦‚æœç« èŠ‚æ˜¯'å¼•è¨€'æˆ–'ç»“è®º'ï¼Œè¯·åªæ’°å†™å†…å®¹ï¼Œä¸è¦æ·»åŠ äºŒçº§æ ‡é¢˜ã€‚"
        "å†™ä½œæ—¶ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ã€æ¥æº Xã€‘æ ‡è®°ã€‚"
        "åªè¾“å‡ºç« èŠ‚å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯„è®ºæˆ–å¼•å¯¼è¯ã€‚"
    )

    user_prompt = (
        f"æŠ¥å‘Šä¸»é¢˜: {state['topic']}\n"
        f"å½“å‰ç« èŠ‚æ ‡é¢˜: {chapter_title}\n\n"
        f"ç« èŠ‚å†™ä½œä¸Šä¸‹æ–‡:\n{refined_context}"
    )

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    # ç»„è£…å®Œæ•´çš„ç« èŠ‚å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜
    if chapter_title in ["å¼•è¨€", "ç»“è®º"]:
        chapter_content = response.content.strip()
    else:
        chapter_content = f"## {chapter_title}\n\n{response.content.strip()}"

    # å°†å®Œæˆçš„ç« èŠ‚å†…å®¹æ·»åŠ åˆ°æŠ¥å‘Šéƒ¨åˆ†åˆ—è¡¨
    new_sections = state["report_sections"] + [chapter_content]

    # è®¾ç½®ä¸‹ä¸€ä¸ªç« èŠ‚
    next_chapter = state["remaining_chapters"].pop(0) if state["remaining_chapters"] else ""

    print(f"âœ… [å®Œæˆ] ç« èŠ‚ '{chapter_title}' å†™å…¥å®Œæ¯•ã€‚")
    return {
        "report_sections": new_sections,
        "current_chapter": next_chapter,
        "remaining_chapters": state["remaining_chapters"],
    }


async def finalize_report(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 9ï¼šæœ€ç»ˆæŠ¥å‘Šæ•´åˆã€‘
    å°†æ‰€æœ‰ç« èŠ‚å†…å®¹å’Œå‚è€ƒèµ„æ–™æ•´åˆä¸ºæœ€ç»ˆæŠ¥å‘Šã€‚
    """
    print("\nğŸ“ [æ•´åˆ] æ­£åœ¨æ•´åˆæ‰€æœ‰ç« èŠ‚å’Œå‚è€ƒèµ„æ–™...")

    # 1. æå–æ‰€æœ‰å¼•ç”¨
    full_text = "\n\n".join(state["report_sections"]) + "\n\n" + "\n\n".join(state["all_findings"])

    # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ˜ å°„ (V4/V5/V6/V7 æ ¼å¼ï¼šå¼•ç”¨: {..., "['1']": "url"})
    citation_pattern = re.compile(r"å¼•ç”¨: (\{.*?\}|\{.*?\})", re.DOTALL)

    unique_references = {}

    for match in citation_pattern.finditer(full_text):
        try:
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„JSONè§£æï¼Œå®é™…éœ€è¦æ›´å¥å£®çš„é€»è¾‘
            citation_str = match.group(1).replace("'", '"')
            citation_map = eval(citation_str)  # ä½¿ç”¨ eval ç®€åŒ–ï¼Œä½†åœ¨ç”Ÿäº§ä¸­åº”é¿å…
            unique_references.update(citation_map)
        except Exception as e:
            # å¿½ç•¥è§£æå¤±è´¥çš„å¼•ç”¨
            continue

    # 2. æ ¼å¼åŒ–å‚è€ƒèµ„æ–™
    references_list = []
    if unique_references:
        references_list.append("## å‚è€ƒèµ„æ–™ (Citations)")
        # æŒ‰ç…§å¼•ç”¨ ID æ’åº (e.g., [1], [2]...)
        # æ³¨æ„: è¿™é‡Œçš„é”®å¯èƒ½åŒ…å« '### é€’å½’é•¿æœŸè®°å¿†æ‘˜è¦:'ï¼Œéœ€è¦è¿‡æ»¤
        valid_refs = [(k, v) for k, v in unique_references.items() if k.startswith('[') and k.endswith(']')]

        # ä½¿ç”¨å®‰å…¨çš„æ’åºï¼Œç¡®ä¿é”®æ˜¯æ•°å­—
        def get_sort_key(item):
            try:
                return int(item[0].strip('[]'))
            except ValueError:
                return float('inf')  # å°†éæ•°å­—é”®æ”¾åœ¨æœ€å

        sorted_refs = sorted(valid_refs, key=get_sort_key)

        for source_id, url in sorted_refs:
            references_list.append(f"{source_id} {url}")

    # 3. ç»„åˆæœ€ç»ˆæŠ¥å‘Š
    report_title = f"# {state['topic']} æ·±åº¦ç ”ç©¶æŠ¥å‘Š\n\n"
    final_report = report_title + "\n\n".join(state["report_sections"]) + "\n\n" + "\n".join(references_list)

    print("âœ… [å®Œæˆ] æœ€ç»ˆæŠ¥å‘Šæ•´åˆå®Œæ¯•ã€‚")
    return {"final_report": final_report}


# ==============================================================================
# 5. æ„å»ºå›¾é€»è¾‘ (Routing Logic)
# ==============================================================================

def should_continue_research(state: ResearchState):
    """
    æ¡ä»¶è¾¹é€»è¾‘ï¼šå†³å®šæ˜¯å›å»æ¥ç€æœï¼Œè¿˜æ˜¯è¿›å…¥å†™ä½œæµç¨‹
    """
    missing = state.get("missing_info", "")
    if missing == "sufficient" or state["loop_count"] >= MAX_LOOPS:
        return "to_outline"
    else:
        return "to_generator"


def should_continue_writing(state: ResearchState):
    """
    æ¡ä»¶è¾¹é€»è¾‘ï¼šå†³å®šæ˜¯ç»§ç»­å†™ä¸‹ä¸€ç« ï¼Œè¿˜æ˜¯ç»“æŸå†™ä½œ
    """
    if state["current_chapter"]:
        return "continue_chapter"
    else:
        return "finalize"


# åˆå§‹åŒ–å›¾
workflow = StateGraph(ResearchState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("planner", plan_research)
workflow.add_node("researcher", execute_search)
workflow.add_node("summarizer", recursive_summarizer)  # [V7 æ–°å¢] é€’å½’æ‘˜è¦èŠ‚ç‚¹
workflow.add_node("evaluator", evaluate_findings)
workflow.add_node("query_generator", generate_new_queries)
workflow.add_node("outline_planner", outline_report)
workflow.add_node("parse_chapters", parse_outline_and_prepare_chapters)
workflow.add_node("chapter_context_retriever", chapter_context_retriever)
workflow.add_node("chapter_writer", chapter_writer)
workflow.add_node("finalizer", finalize_report)

# æ„å»ºæµç¨‹ï¼šç ”ç©¶é˜¶æ®µ
workflow.set_entry_point("planner")
workflow.add_edge("planner", "researcher")

# V7 æµç¨‹ä¿®æ”¹ï¼šæœç´¢ -> æ‘˜è¦ -> è¯„ä¼°
workflow.add_edge("researcher", "summarizer")  # æœç´¢ç»“æœå…ˆè¿›å…¥æ‘˜è¦å‹ç¼©
workflow.add_edge("summarizer", "evaluator")

# è¯„ä¼° -> æ¡ä»¶åˆ¤æ–­
workflow.add_conditional_edges(
    "evaluator",
    should_continue_research,
    {
        "to_generator": "query_generator",
        "to_outline": "outline_planner"
    }
)

# è¿­ä»£å¾ªç¯
workflow.add_edge("query_generator", "researcher")

# ç»“æ„åŒ–å†™ä½œé˜¶æ®µ
workflow.add_edge("outline_planner", "parse_chapters")
workflow.add_edge("parse_chapters", "chapter_context_retriever")
workflow.add_edge("chapter_context_retriever", "chapter_writer")

# ç« èŠ‚å†™ä½œ -> æ¡ä»¶åˆ¤æ–­
workflow.add_conditional_edges(
    "chapter_writer",
    should_continue_writing,
    {
        "continue_chapter": "chapter_context_retriever",  # å¾ªç¯åˆ°ä¸‹ä¸€ç« 
        "finalize": "finalizer"  # ç»“æŸå†™ä½œ
    }
)

workflow.add_edge("finalizer", END)

app = workflow.compile()


# ==============================================================================
# 6. è¿è¡Œå…¥å£
# ==============================================================================

async def run_agent():
    print("=== Deep Research Agent V7 (é€’å½’è®°å¿†å‹ç¼©æ¶æ„) ===")
    topic = input("è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜: ")
    if not topic: topic = "é‡å­è®¡ç®—æœºåœ¨2024å¹´çš„æœ€æ–°çªç ´"

    initial_state = {"topic": topic}

    final_state = await app.ainvoke(initial_state)

    print("\n" + "=" * 50)
    print("æœ€ç»ˆæŠ¥å‘Š:")
    print(final_state["final_report"])

    # ä¿å­˜æ–‡ä»¶
    with open("deep_research_v7.md", "w", encoding="utf-8") as f:
        f.write(final_state["final_report"])
    print("\n[ç³»ç»Ÿ] æŠ¥å‘Šå·²ä¿å­˜è‡³ deep_research_v7.md")


if __name__ == "__main__":
    asyncio.run(run_agent())
