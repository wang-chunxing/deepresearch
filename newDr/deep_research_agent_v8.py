import os
import operator
import asyncio
import re
import json
import requests
from typing import Annotated, List, TypedDict, Union

# å¼•å…¥ LangChain å’Œ LangGraph ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
load_dotenv()

# ==============================================================================
# 1. é…ç½® API Key å’Œ æœ¬åœ°çŸ¥è¯†åº“
# ==============================================================================
api_key = os.environ.get("ARK_API_KEY") or os.environ.get("DOUBAO_API_KEY")
tavily_key = os.environ.get("TAVILY_API_KEY")
kb_domain = os.environ.get("KNOWLEDGE_BASE_DOMAIN") or os.environ.get("KB_DOMAIN") or ""
kb_apikey = os.environ.get("KNOWLEDGE_BASE_API_KEY") or os.environ.get("KB_API_KEY") or ""
kb_service_id = os.environ.get("KNOWLEDGE_BASE_SERVICE_ID") or os.environ.get("KB_SERVICE_ID") or ""

if not api_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°è±†åŒ… API Key/Endpointï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ LLMã€‚")
if not tavily_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚")
if not (kb_domain and kb_apikey and kb_service_id):
    print("â„¹ï¸  æç¤º: æœªæ£€æµ‹åˆ°çŸ¥è¯†åº“é…ç½® (KNOWLEDGE_BASE_DOMAIN/KNOWLEDGE_BASE_API_KEY/KNOWLEDGE_BASE_SERVICE_ID)ã€‚")

# [V8 æ–°å¢] æ¨¡æ‹Ÿæœ¬åœ°çŸ¥è¯†åº“ (RAG æºå¤´)
SIMULATED_LOCAL_KNOWLEDGE_BASE = """
--- æœ¬åœ°çŸ¥è¯†åº“æ–‡æ¡£ (Project Titan Internal Report Q3 2024) ---
1. Project Titan åœ¨ 2024 å¹´ç¬¬ä¸‰å­£åº¦æˆåŠŸå®Œæˆäº† B è½®èèµ„ï¼Œæ€»é¢è¾¾åˆ° 5000 ä¸‡ç¾å…ƒã€‚
2. æ ¸å¿ƒæŠ€æœ¯ 'X-Engine' çš„ä»£ç åº“åœ¨ Q3 ç»å†äº† 3 æ¬¡é‡å¤§æ¶æ„é‡æ„ï¼Œä½¿å…¶é€Ÿåº¦æå‡äº† 40%ã€‚
3. ç«äº‰å¯¹æ‰‹ "Jupiter Tech" åœ¨åŒä¸€æ—¶æœŸæ¨å‡ºäº†å»‰ä»·æ›¿ä»£å“ï¼Œå¯¼è‡´ Titan åœ¨ä¸œå—äºšå¸‚åœºä»½é¢ä¸‹é™äº† 5%ã€‚
4. å¸‚åœºé¢„æµ‹ï¼šå¾—ç›Šäº X-Engine çš„æ€§èƒ½æå‡ï¼ŒTitan çš„ 2025 å¹´è¥æ”¶é¢„è®¡å¢é•¿ 70%ï¼Œå‰ææ˜¯æˆåŠŸè¿›å…¥æ¬§æ´²å¸‚åœºã€‚
--- End of Local Docs ---
"""
if SIMULATED_LOCAL_KNOWLEDGE_BASE.strip() == "":
    print("ğŸ”” æœ¬åœ°çŸ¥è¯†åº“ä¸ºç©ºã€‚RAG èŠ‚ç‚¹å°†è¢«æœ‰æ•ˆè·³è¿‡ã€‚")


# ==============================================================================
# 2. å®šä¹‰çŠ¶æ€ (State) - V9 æ¶æ„ (å…±äº«ä¸Šä¸‹æ–‡)
# ==============================================================================
class ResearchState(TypedDict):
    topic: str  # åŸå§‹ç ”ç©¶ä¸»é¢˜
    topic_category: str  # ä¸»é¢˜ç±»å‹
    current_queries: List[str]  # å½“å‰è¿™ä¸€è½®éœ€è¦æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢
    all_findings: List[str]  # ç´¯ç§¯æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ (åŒ…å« web å‘ç°, é€’å½’è®°å¿†, å’Œ local RAG ç»“æœ)

    # å†™ä½œè¿­ä»£çŠ¶æ€ - è¿™æ˜¯æ‰€æœ‰ Agent å…±äº«çš„â€œå†³ç­–è½¨è¿¹â€
    report_outline: str  # æŠ¥å‘Šçš„å®Œæ•´ç»“æ„å¤§çº² (Markdown string)
    remaining_chapters: List[str]  # å¾…å†™ä½œçš„ç« èŠ‚æ ‡é¢˜åˆ—è¡¨
    current_chapter: str  # æ­£åœ¨å¤„ç†çš„ç« èŠ‚æ ‡é¢˜
    refined_context: str  # ç»è¿‡ç²¾ç‚¼å’Œç­›é€‰çš„**å½“å‰ç« èŠ‚**å†™ä½œä¸Šä¸‹æ–‡
    report_sections: List[str]  # **[å…³é”®å…±äº«ä¸Šä¸‹æ–‡]** å·²å®Œæˆçš„ç« èŠ‚å†…å®¹ï¼ˆMarkdownï¼‰

    loop_count: int  # å½“å‰è¿­ä»£æ¬¡æ•° (é˜²æ­¢æ­»å¾ªç¯)
    missing_info: str  # è¯„ä¼°é˜¶æ®µå‘ç°çš„ç¼ºå¤±ä¿¡æ¯ (ç”¨äºæŒ‡å¯¼ä¸‹ä¸€è½®)
    final_report: str  # æœ€ç»ˆæŠ¥å‘Š


# ==============================================================================
# 3. åˆå§‹åŒ–æ¨¡å‹å’Œå·¥å…·
# ==============================================================================
# è±†åŒ…æ¨¡å‹ (æ‰€æœ‰èŠ‚ç‚¹éƒ½ä½¿ç”¨å¼‚æ­¥è°ƒç”¨)
class _LLMResponse:
    def __init__(self, content: str):
        self.content = content

class _DummyLLM:
    async def ainvoke(self, messages):
        sys = messages[0].content if messages else ""
        human = messages[-1].content if messages else ""
        if "æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢" in sys or "æŸ¥è¯¢åˆ—è¡¨" in sys:
            topic = human.split("ä¸»é¢˜:")[-1].strip() if "ä¸»é¢˜:" in human else "ä¸»é¢˜"
            return _LLMResponse(f"{topic} å®šä¹‰\n{topic} ç°çŠ¶\n{topic} è¶‹åŠ¿")
        if "å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€" in sys:
            return _LLMResponse("æŠ€æœ¯ç»¼è¿°")
        if "RAG æ£€ç´¢å™¨" in sys:
            return _LLMResponse("### æ¥è‡ªæœ¬åœ°çŸ¥è¯†åº“çš„ RAG å‘ç°:\nè¦ç‚¹ã€æœ¬åœ°çŸ¥è¯† 1ã€‘")
        if "æå–å…³é”®äº‹å®" in sys:
            return _LLMResponse("è¦ç‚¹1ã€æ¥æº 1ã€‘\nè¦ç‚¹2ã€æ¥æº 2ã€‘\n---\nå¼•ç”¨: {'[1]': 'https://example.com/1', '[2]': 'https://example.com/2'}")
        if "è‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆ" in sys:
            return _LLMResponse("SUFFICIENT")
        if "é«˜çº§æŠ¥å‘Šç»“æ„å¸ˆ" in sys:
            return _LLMResponse("## èƒŒæ™¯\n## ç°çŠ¶\n## ç«äº‰æ ¼å±€\n## è¶‹åŠ¿")
        if "é€’å½’è®°å¿†å‹ç¼©" in sys:
            return _LLMResponse("### é€’å½’é•¿æœŸè®°å¿†æ‘˜è¦:\nç»¼åˆè¦ç‚¹ã€æ¥æº 1ã€‘")
        if "ä¸Šä¸‹æ–‡å‹ç¼©ä¸“å®¶" in sys and "ç« èŠ‚æ ‡é¢˜" in human:
            return _LLMResponse("ä¸ç« èŠ‚ç›´æ¥ç›¸å…³çš„è¦ç‚¹ã€æ¥æº 1ã€‘")
        if "ä¸“ä¸šæŠ¥å‘Šæ’°ç¨¿äºº" in sys:
            return _LLMResponse("æ®µè½å†…å®¹ï¼ŒåŒ…å«å¼•ç”¨ã€æ¥æº 1ã€‘")
        return _LLMResponse("ç¤ºä¾‹è¾“å‡º")

class _DummySearch:
    def __init__(self, max_results: int = 3):
        self.max_results = max_results
    async def ainvoke(self, query: str):
        return [{"url": f"https://example.com/{i}", "content": f"ä¸{query}ç›¸å…³çš„ç¤ºä¾‹å†…å®¹ {i}"} for i in range(1, self.max_results + 1)]

llm = ChatOpenAI(
    model="doubao-seed-1-6-251015",
    api_key=api_key,
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    temperature=0.1,
) if api_key else _DummyLLM()

search_tool = TavilySearchResults(max_results=3) if tavily_key else _DummySearch(max_results=3)

# æœ€å¤§è¿­ä»£æ¬¡æ•°
MAX_LOOPS = 3
# è®°å¿†å‹ç¼©é˜ˆå€¼ (all_findings è¶…è¿‡æ­¤æ•°é‡åè§¦å‘å‹ç¼©)
COMPRESSION_THRESHOLD = 6


# ==============================================================================
# 4. å®šä¹‰èŠ‚ç‚¹é€»è¾‘ (Nodes) - æ ¸å¿ƒé€»è¾‘ä¸å˜ï¼Œä½† `chapter_writer` å¢å¼º
# ==============================================================================

async def api_call_with_retry(llm_input, max_retries=3):
    """ç”¨äºæ‰€æœ‰ LLM è°ƒç”¨çš„é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            return await llm.ainvoke(llm_input)
        except Exception as e:
            if attempt < max_retries - 1:
                delay = 2 ** attempt
                print(f"   âš ï¸ API è°ƒç”¨å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {delay}s...")
                await asyncio.sleep(delay)
            else:
                raise e
    return None


async def plan_research(state: ResearchState):
    """ã€èŠ‚ç‚¹ 1ï¼šåˆå§‹è§„åˆ’ä¸åˆ†ç±»ã€‘"""
    print(f"\nğŸš€ [å¯åŠ¨] å¼€å§‹ç ”ç©¶ä¸»é¢˜: {state['topic']}")

    planning_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„ä¸»é¢˜æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢ã€‚æŸ¥è¯¢åº”æ¶µç›–åŸºç¡€å®šä¹‰ã€ç°çŠ¶å’Œä¸»è¦äº‰è®®ç‚¹ã€‚åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚")
    queries_response = await api_call_with_retry([
        SystemMessage(content=planning_prompt), HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    queries = [line.strip() for line in queries_response.content.split('\n') if line.strip()][:3]

    categorization_prompt = (
        "æ ¹æ®ç”¨æˆ·çš„ä¸»é¢˜ï¼Œå°†å…¶å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š[æŠ€æœ¯ç»¼è¿°, å¸‚åœºåˆ†æ, ç»æµè¶‹åŠ¿, å†å²äº‹ä»¶, äººç‰©ä¼ è®°, è¡Œä¸šæŠ¥å‘Š, æ¦‚å¿µè§£é‡Š]ã€‚è¯·åªè¿”å›æœ€åˆé€‚çš„ç±»åˆ«åç§°ï¼Œä¸å¸¦ä»»ä½•è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·ã€‚")
    category_response = await api_call_with_retry([
        SystemMessage(content=categorization_prompt), HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    topic_category = category_response.content.strip()

    print(f"ğŸ“‹ [è§„åˆ’] ä¸»é¢˜ç±»å‹: {topic_category} | åˆå§‹æŸ¥è¯¢: {queries}")

    return {
        "current_queries": queries, "topic_category": topic_category, "all_findings": [],
        "loop_count": 0, "missing_info": "", "report_sections": []
    }


async def retrieve_local_knowledge(state: ResearchState):
    """ã€èŠ‚ç‚¹ 1.5ï¼šæœ¬åœ°çŸ¥è¯†æ£€ç´¢ (RAG)ã€‘ä»…è°ƒç”¨çœŸå®çŸ¥è¯†åº“ï¼Œç¼ºå¤±åˆ™è·³è¿‡"""
    queries = state["current_queries"]
    query_str = "\n".join(queries)

    # å°è¯•çœŸå®çŸ¥è¯†åº“è°ƒç”¨
    if kb_domain and kb_apikey and kb_service_id:
        print("\nğŸ§  [æœ¬åœ°æ£€ç´¢] æ­£åœ¨è°ƒç”¨çœŸå®çŸ¥è¯†åº“æœåŠ¡ (Volcengine KB)...")
        headers = {
            "Accept": "application/json",
            "Content-Type": "application/json;charset=UTF-8",
            "Host": kb_domain,
            "Authorization": f"Bearer {kb_apikey}",
        }
        payload = {
            "service_resource_id": kb_service_id,
            "messages": [{"role": "user", "content": query_str}],
            "stream": False,
        }
        url = f"http://{kb_domain}/api/knowledge/service/chat"
        try:
            rsp = await asyncio.to_thread(requests.post, url, headers=headers, data=json.dumps(payload), timeout=15)
            content_type = rsp.headers.get("Content-Type", "")
            rag_finding = None
            if content_type.startswith("application/json"):
                try:
                    data = rsp.json()
                    items = []
                    citation_map = {}
                    if isinstance(data, dict):
                        result_list = (((data.get("data") or {}) ).get("result_list") or [])
                        for idx, item in enumerate(result_list, start=1):
                            c = (item.get("content") or "").strip()
                            if c:
                                tag = f"[KB{idx}]"
                                items.append(f"- {c}ã€KB{idx}ã€‘")
                                citation_map[tag] = f"kb://{item.get('id') or idx}"
                    if items:
                        rag_finding = "### æ¥è‡ªæœ¬åœ°çŸ¥è¯†åº“çš„ RAG å‘ç°:\n" + "\n".join(items) + "\n---\nå¼•ç”¨: " + json.dumps(citation_map, ensure_ascii=False)
                    else:
                        rag_finding = "### æ¥è‡ªæœ¬åœ°çŸ¥è¯†åº“çš„ RAG å‘ç°:\n(æ— å†…å®¹)"
                except Exception:
                    rag_finding = f"### æ¥è‡ªæœ¬åœ°çŸ¥è¯†åº“çš„ RAG å‘ç°:\n{rsp.text or ''}"
            else:
                rag_finding = f"### æ¥è‡ªæœ¬åœ°çŸ¥è¯†åº“çš„ RAG å‘ç°:\n{rsp.text or ''}"
            print("  âœ… çœŸå®çŸ¥è¯†åº“è¿”å›æˆåŠŸã€‚")
            new_findings = [rag_finding]
            total_findings = state.get("all_findings", []) + new_findings
            return {"all_findings": total_findings}
        except Exception as e:
            print(f"  âŒ çœŸå®çŸ¥è¯†åº“è°ƒç”¨å¤±è´¥: {e}ï¼Œå°†å›é€€è‡³æ¨¡æ‹ŸçŸ¥è¯†åº“ã€‚")

    print("â­ï¸ [æœ¬åœ°æ£€ç´¢] æœªé…ç½®çœŸå®çŸ¥è¯†åº“ï¼Œè·³è¿‡æœ¬åœ°æ£€ç´¢ã€‚")
    return {}


async def execute_search(state: ResearchState):
    """ã€èŠ‚ç‚¹ 2ï¼šæ‰§è¡Œæœç´¢ã€‘"""
    loop_idx = state["loop_count"] + 1
    queries = state["current_queries"]
    print(f"\nğŸ” [ç¬¬ {loop_idx} è½®æœç´¢] æ­£åœ¨å¹¶å‘æ‰§è¡Œ {len(queries)} ä¸ªæŸ¥è¯¢...")

    async def process_query(query):
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ªæŸ¥è¯¢å’Œæ€»ç»“çš„å­ä»»åŠ¡"""
        try:
            search_results = await search_tool.ainvoke(query)
            context_blocks, citation_map = [], {}
            for i, res in enumerate(search_results):
                citation_map[f"[{i + 1}]"] = res['url']
                context_blocks.append(f"ã€æ¥æº {i + 1}ã€‘: {res['content']}")
            context = "\n".join(context_blocks)

            summary_prompt = (
                f"é’ˆå¯¹æŸ¥è¯¢ '{query}'ï¼Œä»ä»¥ä¸‹ã€æ¥æºã€‘ä¸­æå–å…³é”®äº‹å®ã€æ•°æ®å’Œè§‚ç‚¹ã€‚"
                "åœ¨æ‘˜è¦ä¸­ï¼ŒåŠ¡å¿…ä½¿ç”¨æ ¼å¼ã€æ¥æº Xã€‘å¼•ç”¨ä½ ä½¿ç”¨çš„ä»»ä½•ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š'æŠ•èµ„å¢é•¿äº†30%ã€æ¥æº 2ã€‘'ã€‚"
                "å¿½ç•¥æ— å…³ä¿¡æ¯ã€‚ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ï¼Œå¹¶åˆ—å‡ºå®Œæ•´çš„å¼•ç”¨æ˜ å°„ã€‚"
                f"æœ€ç»ˆè¿”å›æ ¼å¼ï¼š\n---\næ‘˜è¦å†…å®¹\n---\nå¼•ç”¨: {citation_map}"
            )

            summary_response = await api_call_with_retry(
                [SystemMessage(content=summary_prompt), HumanMessage(content=context)])

            return f"### å…³äº '{query}' çš„å‘ç° (ç¬¬ {loop_idx} è½®):\n{summary_response.content}"

        except Exception as e:
            print(f"  âŒ æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
            return None

    tasks = [process_query(query) for query in queries]
    results = await asyncio.gather(*tasks)

    new_findings = [r for r in results if r]
    total_findings = state["all_findings"] + new_findings

    return {"all_findings": total_findings, "loop_count": loop_idx}


async def recursive_summarizer(state: ResearchState):
    """ã€èŠ‚ç‚¹ 2.5ï¼šé€’å½’è®°å¿†å‹ç¼©ã€‘ (V7/V8 æœºåˆ¶)"""
    findings = state["all_findings"]
    if len(findings) < COMPRESSION_THRESHOLD:
        print("ğŸ’¡ [è®°å¿†] å‘ç°é¡¹ä¸è¶³ï¼Œè·³è¿‡é€’å½’æ‘˜è¦ã€‚")
        return {}

    split_point = len(findings) // 2
    old_findings_to_compress = findings[:split_point]
    new_findings_to_keep = findings[split_point:]

    print(f"\nğŸ§  [è®°å¿†å‹ç¼©] å‘ç° {len(findings)} é¡¹ï¼Œæ­£åœ¨å‹ç¼©å‰ {split_point} é¡¹ (æ—§è®°å¿†)...")
    context_to_summarize = "\n\n---\n\n".join(old_findings_to_compress)

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªé€’å½’è®°å¿†å‹ç¼©å¼•æ“ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸‹æ–¹æä¾›çš„æ—§ç ”ç©¶å‘ç°ï¼ˆåŒ…å« web å‘ç°å’Œæœ¬åœ° RAG ç»“æœï¼‰ï¼Œå‹ç¼©æˆä¸€ä¸ª**å•ä¸€ã€é«˜å±‚æ¬¡ã€æµ“ç¼©çš„æ‘˜è¦**ã€‚"
        "ä¿ç•™æ ¸å¿ƒäº‹å®å’Œè¶‹åŠ¿ï¼Œç§»é™¤ä¸å¿…è¦çš„ç»†èŠ‚ã€‚æ‘˜è¦å¿…é¡»ä»¥ '### é€’å½’é•¿æœŸè®°å¿†æ‘˜è¦:' å¼€å¤´ã€‚"
        "è¯·åŠ¡å¿…åœ¨æ‘˜è¦ä¸­ä¿ç•™æ‰€æœ‰åŸå§‹çš„ã€æ¥æº Xã€‘æˆ–ã€æœ¬åœ°çŸ¥è¯† Xã€‘å¼•ç”¨æ ‡è®°ã€‚åªè¿”å›æ‘˜è¦å†…å®¹ã€‚"
    )

    response = await api_call_with_retry(
        [SystemMessage(content=system_prompt), HumanMessage(content=context_to_summarize)])

    compressed_summary = response.content.strip()
    new_all_findings = [compressed_summary] + new_findings_to_keep

    print(f"âœ… [è®°å¿†] è®°å¿†å‹ç¼©å®Œæˆã€‚å½“å‰å‘ç°é¡¹æ•°é‡: {len(new_all_findings)}")

    return {"all_findings": new_all_findings}


async def evaluate_findings(state: ResearchState):
    """ã€èŠ‚ç‚¹ 3ï¼šè¯„ä¼°ä¸åæ€ã€‘"""
    print("\nğŸ¤” [è¯„ä¼°] æ­£åœ¨æ£€æŸ¥èµ„æ–™å®Œæ•´æ€§...")
    topic = state["topic"]
    findings_text = "\n\n".join(state["all_findings"])
    loop_count = state["loop_count"]

    if loop_count >= MAX_LOOPS:
        print("ğŸ›‘ [è¯„ä¼°] å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢æœç´¢ã€‚")
        return {"missing_info": "sufficient"}

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªè‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆã€‚è¯·é˜…è¯»ç›®å‰æ”¶é›†åˆ°çš„ç¬”è®°ï¼ˆåŒ…æ‹¬é€’å½’æ‘˜è¦ã€æœ¬åœ° RAG ç»“æœå’Œç½‘ç»œå‘ç°ï¼‰ï¼Œåˆ¤æ–­æ˜¯å¦è¶³ä»¥æ’°å†™å…³äºè¯¥ä¸»é¢˜çš„æ·±åº¦æŠ¥å‘Šã€‚"
        "å¦‚æœèµ„æ–™å……è¶³ï¼Œè¯·åªå›å¤ 'SUFFICIENT'ã€‚"
        "å¦‚æœèµ„æ–™ç¼ºå¤±ï¼ˆä¾‹å¦‚ç¼ºå°‘å…·ä½“æ•°æ®ã€åé¢è§‚ç‚¹ã€æœ€æ–°è¿›å±•ï¼‰ï¼Œè¯·å›å¤ 'MISSING: <ç¼ºå¤±å†…å®¹çš„æè¿°>'ã€‚"
    )

    response = await api_call_with_retry([SystemMessage(content=system_prompt),
                                          HumanMessage(content=f"ç ”ç©¶ä¸»é¢˜: {topic}\n\nç›®å‰ç¬”è®°:\n{findings_text}")])

    if "SUFFICIENT" in response.content.upper():
        print("âœ… [è¯„ä¼°] èµ„æ–™å·²å……è¶³ï¼")
        return {"missing_info": "sufficient"}
    else:
        print(f"âš ï¸ [è¯„ä¼°] å‘ç°ç¼ºå£: {response.content}")
        return {"missing_info": response.content}


async def generate_new_queries(state: ResearchState):
    """ã€èŠ‚ç‚¹ 4ï¼šç”Ÿæˆè¡¥å……æŸ¥è¯¢ã€‘"""
    missing_info = state["missing_info"]
    print("\nğŸ”„ [è¿­ä»£] æ­£åœ¨ç”Ÿæˆè¡¥å……æŸ¥è¯¢ä»¥å¡«è¡¥ç¼ºå£...")

    system_prompt = ("æ ¹æ®ç¼ºå¤±çš„ä¿¡æ¯æè¿°ï¼Œç”Ÿæˆ 2 ä¸ªå…·ä½“çš„æœç´¢å¼•æ“æŸ¥è¯¢è¯­å¥æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚")
    response = await api_call_with_retry(
        [SystemMessage(content=system_prompt), HumanMessage(content=f"ç¼ºå¤±ä¿¡æ¯: {missing_info}")])

    new_queries = [line.strip() for line in response.content.split('\n') if line.strip()][:2]
    print(f"ğŸ†• [è¡¥å……æŸ¥è¯¢] {new_queries}")

    return {"current_queries": new_queries}


async def outline_report(state: ResearchState):
    """ã€èŠ‚ç‚¹ 5ï¼šåŠ¨æ€ç”ŸæˆæŠ¥å‘Šå¤§çº²ã€‘"""
    print("\nğŸ“ [ç»“æ„] æ­£åœ¨ç”ŸæˆåŠ¨æ€æŠ¥å‘Šå¤§çº²...")

    topic = state["topic"]
    category = state["topic_category"]
    findings_preview = "\n\n".join(state["all_findings"])

    system_prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªé«˜çº§æŠ¥å‘Šç»“æ„å¸ˆã€‚ä¸»é¢˜ç±»åˆ«æ˜¯ '{category}'ã€‚"
        "è¯·æ ¹æ®è¿™ä¸ªç±»åˆ«å’Œä»¥ä¸‹åˆæ­¥ç ”ç©¶ç¬”è®°ï¼Œç”Ÿæˆä¸€ä»½æœ€ä¸“ä¸šã€æœ€ç›¸å…³çš„æŠ¥å‘Šå¤§çº²ã€‚"
        "å¤§çº²åº”è‡³å°‘åŒ…å« 4 ä¸ªä¸»è¦ç« èŠ‚ï¼ˆMarkdown äºŒçº§æ ‡é¢˜ ##ï¼‰ï¼Œå¹¶ç›´æ¥è¿”å› Markdown æ ¼å¼çš„å¤§çº²ã€‚"
        "æ³¨æ„ï¼šä¸è¦åœ¨äºŒçº§æ ‡é¢˜ä¸­åŒ…å« 'å¼•è¨€' æˆ– 'ç»“è®º'ï¼Œç•™ç»™åé¢çš„èŠ‚ç‚¹å¤„ç†ã€‚"
    )

    user_prompt = f"ç ”ç©¶ä¸»é¢˜: {topic}\nä¸»é¢˜ç±»åˆ«: {category}\n\nå…¨éƒ¨ç ”ç©¶ç¬”è®°:\n{findings_preview}"

    response = await api_call_with_retry([SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)])

    print(f"âœ… [ç»“æ„] å¤§çº²å·²ç”Ÿæˆï¼ŒåŸºäºç±»åˆ«: {category}ã€‚")
    return {"report_outline": response.content}


async def parse_outline_and_prepare_chapters(state: ResearchState):
    """ã€èŠ‚ç‚¹ 6ï¼šè§£æå¤§çº²å¹¶å‡†å¤‡ç« èŠ‚ã€‘"""
    print("\nğŸ“š [å‡†å¤‡] æ­£åœ¨è§£æå¤§çº²å¹¶å‡†å¤‡è¿­ä»£å†™ä½œ...")
    outline = state["report_outline"]
    chapter_titles = re.findall(r'##\s*(.*)', outline)
    all_chapters = ["å¼•è¨€"] + chapter_titles + ["ç»“è®º"]

    if not all_chapters:
        return {"remaining_chapters": [], "current_chapter": ""}

    current_chapter = all_chapters.pop(0)
    print(f"â¡ï¸ [å½“å‰ç« èŠ‚] '{current_chapter}' | å‰©ä½™ {len(all_chapters)} ç« å¾…å†™ã€‚")

    return {"remaining_chapters": all_chapters, "current_chapter": current_chapter}


async def chapter_context_retriever(state: ResearchState):
    """ã€èŠ‚ç‚¹ 7: ç« èŠ‚ä¸Šä¸‹æ–‡æ£€ç´¢å™¨ã€‘"""
    print(f"\nâœ‚ï¸ [æ£€ç´¢] æ­£åœ¨ä¸ºç« èŠ‚ '{state['current_chapter']}' æç‚¼æ ¸å¿ƒä¸Šä¸‹æ–‡...")

    chapter_title = state["current_chapter"]
    all_findings = "\n\n".join(state["all_findings"])

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å‹ç¼©ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„**ç« èŠ‚æ ‡é¢˜**ï¼Œä»ä¸‹æ–¹æ‰€æœ‰ç ”ç©¶å‘ç°ï¼ˆåŒ…å« web å‘ç°ã€æœ¬åœ° RAG ç»“æœå’Œé•¿æœŸè®°å¿†æ‘˜è¦ï¼‰ä¸­ï¼Œ"
        "ä»…æŒ‘é€‰å‡º**ä¸è¯¥ç« èŠ‚ä¸»é¢˜ç›´æ¥ç›¸å…³**çš„äº‹å®ã€æ•°æ®å’Œå¼•ç”¨ä¿¡æ¯ã€‚"
        "è¯·å°†æç‚¼åçš„ä¿¡æ¯ä»¥ç²¾ç®€ã€ç»“æ„åŒ–çš„æ–¹å¼è¿”å›ï¼Œ**åŠ¡å¿…ä¿ç•™æ‰€æœ‰ã€æ¥æº Xã€‘æˆ–ã€æœ¬åœ°çŸ¥è¯† Xã€‘æ ‡è®°**ã€‚"
    )

    user_prompt = f"ç« èŠ‚æ ‡é¢˜: {chapter_title}\n\nå…¨éƒ¨åŸå§‹ç ”ç©¶å‘ç°:\n{all_findings}"

    response = await api_call_with_retry([SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)])

    return {"refined_context": response.content}


async def chapter_writer(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 8ï¼šç« èŠ‚æ’°å†™å™¨ã€‘ - V9 å¢å¼ºï¼šé›†æˆå·²å®Œæˆç« èŠ‚ä½œä¸ºå…±äº«ä¸Šä¸‹æ–‡ã€‚
    """
    chapter_title = state["current_chapter"]
    refined_context = state["refined_context"]
    previous_sections = state["report_sections"]  # <-- å…±äº«ä¸Šä¸‹æ–‡ï¼šå·²å®Œæˆçš„ç« èŠ‚å†…å®¹

    print(f"\nâœï¸ [å†™ä½œ] æ­£åœ¨æ’°å†™ç« èŠ‚: '{chapter_title}'...")

    # æ„é€ å†å²ç« èŠ‚å†…å®¹ï¼Œç”¨äºä¸Šä¸‹æ–‡
    previous_content = "\n\n---\n\n".join(previous_sections) if previous_sections else "æ— ï¼ˆè¿™æ˜¯æŠ¥å‘Šçš„ç¬¬ä¸€éƒ¨åˆ†ï¼‰"

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæŠ¥å‘Šæ’°ç¨¿äººã€‚è¯·æ ¹æ®æä¾›çš„**ç²¾ç‚¼ä¸Šä¸‹æ–‡**ï¼Œå¹¶å‚è€ƒ**ä¹‹å‰å·²å®Œæˆçš„ç« èŠ‚å†…å®¹**ï¼Œæ’°å†™å…³äºä¸»é¢˜çš„**ä¸€ä¸ªç‹¬ç«‹ç« èŠ‚**ã€‚"
        "ä½ çš„ä¸»è¦ç›®æ ‡æ˜¯ç¡®ä¿æ–°ç« èŠ‚ä¸å†å²å†…å®¹åœ¨**è¯­æ°”ã€é£æ ¼å’Œé€»è¾‘æ‰¿æ¥ä¸Šä¿æŒå®Œç¾è¿è´¯**ï¼Œé¿å…ç”Ÿç¡¬çš„è¿‡æ¸¡æˆ–é‡å¤è§‚ç‚¹ã€‚"
        "å¦‚æœç« èŠ‚æ˜¯'å¼•è¨€'æˆ–'ç»“è®º'ï¼Œè¯·ç›¸åº”è°ƒæ•´å†™ä½œé£æ ¼ã€‚"
        "å¦‚æœç« èŠ‚æ˜¯ä¸»ä½“å†…å®¹ï¼Œè¯·å°†æ ‡é¢˜ä½œä¸º Markdown äºŒçº§æ ‡é¢˜ï¼ˆä¾‹å¦‚ï¼š## ç« èŠ‚æ ‡é¢˜ï¼‰å¼€å¤´ï¼Œç„¶åæ’°å†™å†…å®¹ã€‚"
        "å†™ä½œæ—¶ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ã€æ¥æº Xã€‘ï¼ˆç½‘ç»œï¼‰æˆ–ã€æœ¬åœ°çŸ¥è¯† Xã€‘ï¼ˆæœ¬åœ°çŸ¥è¯†åº“ï¼‰æ ‡è®°ã€‚åªè¾“å‡ºç« èŠ‚å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯„è®ºæˆ–å¼•å¯¼è¯ã€‚"
    )

    user_prompt = (
        f"æŠ¥å‘Šä¸»é¢˜: {state['topic']}\n"
        f"å½“å‰ç« èŠ‚æ ‡é¢˜: {chapter_title}\n\n"
        f"ã€ä¹‹å‰å·²å®Œæˆçš„ç« èŠ‚å†…å®¹ï¼ˆå…±äº«ä¸Šä¸‹æ–‡ï¼‰ã€‘:\n{previous_content}\n\n"
        f"ã€å½“å‰ç« èŠ‚å†™ä½œä¸Šä¸‹æ–‡ã€‘:\n{refined_context}"
    )

    response = await api_call_with_retry([SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)])

    # ç»„è£…å®Œæ•´çš„ç« èŠ‚å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜
    if chapter_title in ["å¼•è¨€", "ç»“è®º"]:
        chapter_content = response.content.strip()
    else:
        chapter_content = f"## {chapter_title}\n\n{response.content.strip()}"

    # å°†å®Œæˆçš„ç« èŠ‚å†…å®¹æ·»åŠ åˆ°æŠ¥å‘Šéƒ¨åˆ†åˆ—è¡¨ (æ›´æ–°å…±äº«ä¸Šä¸‹æ–‡)
    new_sections = previous_sections + [chapter_content]

    # è®¾ç½®ä¸‹ä¸€ä¸ªç« èŠ‚
    next_chapter = state["remaining_chapters"].pop(0) if state["remaining_chapters"] else ""

    print(f"âœ… [å®Œæˆ] ç« èŠ‚ '{chapter_title}' å†™å…¥å®Œæ¯•ã€‚")
    return {
        "report_sections": new_sections,
        "current_chapter": next_chapter,
        "remaining_chapters": state["remaining_chapters"],
    }


async def finalize_report(state: ResearchState):
    """ã€èŠ‚ç‚¹ 9ï¼šæœ€ç»ˆæŠ¥å‘Šæ•´åˆã€‘"""
    print("\nğŸ“ [æ•´åˆ] æ­£åœ¨æ•´åˆæ‰€æœ‰ç« èŠ‚å’Œå‚è€ƒèµ„æ–™...")

    full_text = "\n\n".join(state["report_sections"]) + "\n\n" + "\n\n".join(state["all_findings"])
    citation_pattern = re.compile(r"å¼•ç”¨: (\{.*?\}|\{.*?\})", re.DOTALL)
    unique_references = {}

    for match in citation_pattern.finditer(full_text):
        try:
            citation_str = match.group(1).replace("'", '"')
            citation_map = eval(citation_str)
            unique_references.update(citation_map)
        except Exception:
            continue

    references_list = []

    web_refs = [(k, v) for k, v in unique_references.items() if k.startswith('[') and k.endswith(']')]

    if web_refs:
        references_list.append("## é™„å½• A: ç½‘ç»œå‚è€ƒèµ„æ–™ (Citations)")

        def get_sort_key(item):
            try:
                return int(item[0].strip('[]'))
            except ValueError:
                return float('inf')

        sorted_web_refs = sorted(web_refs, key=get_sort_key)

        for source_id, url in sorted_web_refs:
            references_list.append(f"{source_id} {url}")

    if "ã€æœ¬åœ°çŸ¥è¯†" in full_text:
        references_list.append("\n## é™„å½• B: æœ¬åœ°çŸ¥è¯†åº“æ¥æº (RAG Source)")
        references_list.append(SIMULATED_LOCAL_KNOWLEDGE_BASE)
        references_list.append("\n*æ³¨æ„: æœ¬åœ°çŸ¥è¯†åº“çš„å¼•ç”¨æ ¼å¼ä¸ºã€æœ¬åœ°çŸ¥è¯† Xã€‘*")

    report_title = f"# {state['topic']} æ·±åº¦ç ”ç©¶æŠ¥å‘Š\n\n"
    final_report = report_title + "\n\n".join(state["report_sections"]) + "\n\n" + "\n".join(references_list)

    print("âœ… [å®Œæˆ] æœ€ç»ˆæŠ¥å‘Šæ•´åˆå®Œæ¯•ã€‚")
    return {"final_report": final_report}


# ==============================================================================
# 5. æ„å»ºå›¾é€»è¾‘ (Routing Logic)
# ==============================================================================

def should_continue_research(state: ResearchState):
    """æ¡ä»¶è¾¹é€»è¾‘ï¼šå†³å®šæ˜¯å›å»æ¥ç€æœï¼Œè¿˜æ˜¯è¿›å…¥å†™ä½œæµç¨‹"""
    missing = state.get("missing_info", "")
    if missing == "sufficient" or state["loop_count"] >= MAX_LOOPS:
        return "to_outline"
    else:
        return "to_local_retriever"


def should_continue_writing(state: ResearchState):
    """æ¡ä»¶è¾¹é€»è¾‘ï¼šå†³å®šæ˜¯ç»§ç»­å†™ä¸‹ä¸€ç« ï¼Œè¿˜æ˜¯ç»“æŸå†™ä½œ"""
    if state["current_chapter"]:
        return "continue_chapter"
    else:
        return "finalize"


# åˆå§‹åŒ–å›¾
workflow = StateGraph(ResearchState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("planner", plan_research)
workflow.add_node("knowledge_retriever", retrieve_local_knowledge)
workflow.add_node("researcher", execute_search)
workflow.add_node("summarizer", recursive_summarizer)
workflow.add_node("evaluator", evaluate_findings)
workflow.add_node("query_generator", generate_new_queries)
workflow.add_node("outline_planner", outline_report)
workflow.add_node("parse_chapters", parse_outline_and_prepare_chapters)
workflow.add_node("chapter_context_retriever", chapter_context_retriever)
workflow.add_node("chapter_writer", chapter_writer)  # <-- å¢å¼ºçš„å†™å…¥å™¨
workflow.add_node("finalizer", finalize_report)

# æ„å»ºæµç¨‹ï¼šç ”ç©¶é˜¶æ®µ
workflow.set_entry_point("planner")
workflow.add_edge("planner", "knowledge_retriever")
workflow.add_edge("knowledge_retriever", "researcher")
workflow.add_edge("researcher", "summarizer")
workflow.add_edge("summarizer", "evaluator")

# è¯„ä¼° -> æ¡ä»¶åˆ¤æ–­
workflow.add_conditional_edges(
    "evaluator",
    should_continue_research,
    {
        "to_local_retriever": "query_generator",
        "to_outline": "outline_planner"
    }
)

# è¿­ä»£å¾ªç¯ï¼šç”Ÿæˆæ–°æŸ¥è¯¢ -> æœ¬åœ°æ£€ç´¢
workflow.add_edge("query_generator", "knowledge_retriever")

# ç»“æ„åŒ–å†™ä½œé˜¶æ®µ
workflow.add_edge("outline_planner", "parse_chapters")
workflow.add_edge("parse_chapters", "chapter_context_retriever")
workflow.add_edge("chapter_context_retriever", "chapter_writer")

# ç« èŠ‚å†™ä½œ -> æ¡ä»¶åˆ¤æ–­
workflow.add_conditional_edges(
    "chapter_writer",
    should_continue_writing,
    {
        "continue_chapter": "chapter_context_retriever",
        "finalize": "finalizer"
    }
)

workflow.add_edge("finalizer", END)

app = workflow.compile()


# ==============================================================================
# 6. è¿è¡Œå…¥å£
# ==============================================================================

async def run_agent():
    print("=== Deep Research Agent V9 (å…±äº«å†™ä½œè½¨è¿¹ï¼Œä¿è¯è¿è´¯æ€§) ===")
    topic = input("è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜: ")
    if not topic: topic = "é‡å­è®¡ç®—æœºåœ¨2024å¹´çš„æœ€æ–°çªç ´"

    initial_state = {"topic": topic}

    final_state = await app.ainvoke(initial_state)

    print("\n" + "=" * 50)
    print("æœ€ç»ˆæŠ¥å‘Š:")
    print(final_state["final_report"])

    # ä¿å­˜æ–‡ä»¶
    with open("deep_research_v9.md", "w", encoding="utf-8") as f:
        f.write(final_state["final_report"])
    print("\n[ç³»ç»Ÿ] æŠ¥å‘Šå·²ä¿å­˜è‡³ deep_research_v9.md")


if __name__ == "__main__":
    asyncio.run(run_agent())


# å•ç‚¹çŸ¥è¯†åº“è°ƒç”¨èƒ½åŠ›ï¼ˆä¸è·‘ä¸»æµç¨‹ï¼‰
def kb_service_chat(query: Union[str, list]):
    q = query
    content = q if not isinstance(q, str) else q
    missing = []
    if not kb_domain:
        missing.append("KNOWLEDGE_BASE_DOMAIN")
    if not kb_apikey:
        missing.append("KNOWLEDGE_BASE_API_KEY")
    if not kb_service_id:
        missing.append("KNOWLEDGE_BASE_SERVICE_ID")
    if missing:
        raise ValueError(f"ç¼ºå°‘çŸ¥è¯†åº“é…ç½®: {', '.join(missing)}")

    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json;charset=UTF-8",
        "Host": kb_domain,
        "Authorization": f"Bearer {kb_apikey}",
    }
    payload = {
        "service_resource_id": kb_service_id,
        "messages": [{"role": "user", "content": content}],
        "stream": False,
    }
    url = f"http://{kb_domain}/api/knowledge/service/chat"
    rsp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=15)
    txt = rsp.text or ""
    ctype = rsp.headers.get("Content-Type", "")
    if ctype.startswith("application/json"):
        try:
            return rsp.json()
        except Exception:
            return {"raw": txt, "status": rsp.status_code}
    return {"raw": txt, "status": rsp.status_code}
