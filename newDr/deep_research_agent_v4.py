import os
import operator
import asyncio
from typing import Annotated, List, TypedDict, Union

# å¼•å…¥ LangChain å’Œ LangGraph ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
load_dotenv()

# ==============================================================================
# 1. é…ç½® API Key
# ==============================================================================
api_key = os.environ.get("ARK_API_KEY") or os.environ.get("DOUBAO_API_KEY")
tavily_key = os.environ.get("TAVILY_API_KEY")

if not api_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° ARK/DOUBAO API Keyï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ LLMã€‚")
if not tavily_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚")


# ==============================================================================
# 2. å®šä¹‰çŠ¶æ€ (State) - V5 å‡çº§ç‰ˆ
# ==============================================================================
class ResearchState(TypedDict):
    topic: str  # åŸå§‹ç ”ç©¶ä¸»é¢˜
    topic_category: str  # ä¸»é¢˜ç±»å‹
    current_queries: List[str]  # å½“å‰è¿™ä¸€è½®éœ€è¦æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢
    all_findings: List[str]  # ç´¯ç§¯æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ (åŒ…å«æ‘˜è¦å’Œå¼•ç”¨URL)
    refined_context: str  # [V5 æ–°å¢] ç»è¿‡ç²¾ç‚¼å’Œç­›é€‰çš„å†™ä½œä¸Šä¸‹æ–‡
    loop_count: int  # å½“å‰è¿­ä»£æ¬¡æ•° (é˜²æ­¢æ­»å¾ªç¯)
    missing_info: str  # è¯„ä¼°é˜¶æ®µå‘ç°çš„ç¼ºå¤±ä¿¡æ¯ (ç”¨äºæŒ‡å¯¼ä¸‹ä¸€è½®)
    report_outline: str  # æŠ¥å‘Šçš„ç»“æ„å¤§çº² (åŠ¨æ€ç”Ÿæˆ)
    final_report: str  # æœ€ç»ˆæŠ¥å‘Š


# ==============================================================================
# 3. åˆå§‹åŒ–æ¨¡å‹å’Œå·¥å…·
# ==============================================================================
# è±†åŒ…æ¨¡å‹ (æ‰€æœ‰èŠ‚ç‚¹éƒ½ä½¿ç”¨å¼‚æ­¥è°ƒç”¨)
class _LLMResponse:
    def __init__(self, content: str):
        self.content = content

class _DummyLLM:
    async def ainvoke(self, messages):
        sys = messages[0].content if messages else ""
        human = messages[-1].content if messages else ""
        if "æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢" in sys or "æŸ¥è¯¢åˆ—è¡¨" in sys:
            topic = human.split("ä¸»é¢˜:")[-1].strip() if "ä¸»é¢˜:" in human else "ä¸»é¢˜"
            return _LLMResponse(f"{topic} å®šä¹‰\n{topic} ç°çŠ¶\n{topic} è¶‹åŠ¿")
        if "å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€" in sys:
            return _LLMResponse("æŠ€æœ¯ç»¼è¿°")
        if "æå–å…³é”®äº‹å®" in sys:
            return _LLMResponse("è¦ç‚¹1ã€æ¥æº 1ã€‘\nè¦ç‚¹2ã€æ¥æº 2ã€‘\n---\nå¼•ç”¨: {'[1]': 'https://example.com/1', '[2]': 'https://example.com/2'}")
        if "è‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆ" in sys:
            return _LLMResponse("SUFFICIENT")
        if "é«˜çº§æŠ¥å‘Šç»“æ„å¸ˆ" in sys:
            return _LLMResponse("## èƒŒæ™¯\n## ç°çŠ¶\n## ç«äº‰æ ¼å±€\n## è¶‹åŠ¿")
        if "ä¸Šä¸‹æ–‡å‹ç¼©ä¸“å®¶" in sys:
            return _LLMResponse("## èƒŒæ™¯\nè¦ç‚¹ã€æ¥æº 1ã€‘\n## ç°çŠ¶\nè¦ç‚¹ã€æ¥æº 2ã€‘")
        if "ä¸“ä¸šåˆ†æå¸ˆ" in sys:
            return _LLMResponse("# æŠ¥å‘Š\n\n## èƒŒæ™¯\nå†…å®¹\n\n## ç°çŠ¶\nå†…å®¹\n\n## ç«äº‰æ ¼å±€\nå†…å®¹\n\n## è¶‹åŠ¿\nå†…å®¹\n\n## å‚è€ƒèµ„æ–™\n[1] https://example.com/1\n[2] https://example.com/2")
        return _LLMResponse("ç¤ºä¾‹è¾“å‡º")

class _DummySearch:
    def __init__(self, max_results: int = 3):
        self.max_results = max_results
    async def ainvoke(self, query: str):
        return [{"url": f"https://example.com/{i}", "content": f"ä¸{query}ç›¸å…³çš„ç¤ºä¾‹å†…å®¹ {i}"} for i in range(1, self.max_results + 1)]

llm = ChatOpenAI(
    model="doubao-seed-1-6-251015",
    api_key=api_key,
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    temperature=0.1,
) if api_key else _DummyLLM()

search_tool = TavilySearchResults(max_results=3, tavily_api_key=tavily_key) if tavily_key else _DummySearch(max_results=3)

# æœ€å¤§è¿­ä»£æ¬¡æ•°
MAX_LOOPS = 3


# ==============================================================================
# 4. å®šä¹‰èŠ‚ç‚¹é€»è¾‘ (Nodes) - å…¨éƒ¨æ”¹ä¸º async
# ==============================================================================

async def plan_research(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 1ï¼šåˆå§‹è§„åˆ’ä¸åˆ†ç±»ã€‘
    ç”Ÿæˆåˆå§‹æŸ¥è¯¢å¹¶å¯¹ä¸»é¢˜è¿›è¡Œåˆ†ç±»ï¼Œç”¨äºæŒ‡å¯¼åç»­çš„å¤§çº²ç”Ÿæˆã€‚
    """
    print(f"\nğŸš€ [å¯åŠ¨] å¼€å§‹ç ”ç©¶ä¸»é¢˜: {state['topic']}")

    # æ­¥éª¤ A: ç”Ÿæˆåˆå§‹æŸ¥è¯¢
    planning_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„ä¸»é¢˜æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢ã€‚"
        "æŸ¥è¯¢åº”æ¶µç›–åŸºç¡€å®šä¹‰ã€ç°çŠ¶å’Œä¸»è¦äº‰è®®ç‚¹ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    queries_response = await llm.ainvoke([
        SystemMessage(content=planning_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    queries = [line.strip() for line in queries_response.content.split('\n') if line.strip()][:3]

    # æ­¥éª¤ B: ä¸»é¢˜åˆ†ç±»
    categorization_prompt = (
        "æ ¹æ®ç”¨æˆ·çš„ä¸»é¢˜ï¼Œå°†å…¶å½’ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š[æŠ€æœ¯ç»¼è¿°, å¸‚åœºåˆ†æ, ç»æµè¶‹åŠ¿, å†å²äº‹ä»¶, äººç‰©ä¼ è®°, è¡Œä¸šæŠ¥å‘Š, æ¦‚å¿µè§£é‡Š]ã€‚"
        "è¯·åªè¿”å›æœ€åˆé€‚çš„ç±»åˆ«åç§°ï¼Œä¸å¸¦ä»»ä½•è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·ã€‚"
    )
    category_response = await llm.ainvoke([
        SystemMessage(content=categorization_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])
    topic_category = category_response.content.strip()

    print(f"ğŸ“‹ [è§„åˆ’] ä¸»é¢˜ç±»å‹: {topic_category} | åˆå§‹æŸ¥è¯¢: {queries}")

    # åˆå§‹åŒ–çŠ¶æ€
    return {
        "current_queries": queries,
        "topic_category": topic_category,
        "all_findings": [],
        "loop_count": 0,
        "missing_info": ""
    }


async def execute_search(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 2ï¼šæ‰§è¡Œæœç´¢ã€‘ - å¹¶å‘æœç´¢ï¼Œç¡®ä¿æ‘˜è¦ä¸­åµŒå…¥äº†æ¥æºURLã€‚
    """
    loop_idx = state["loop_count"] + 1
    queries = state["current_queries"]
    print(f"\nğŸ” [ç¬¬ {loop_idx} è½®æœç´¢] æ­£åœ¨å¹¶å‘æ‰§è¡Œ {len(queries)} ä¸ªæŸ¥è¯¢...")

    async def process_query(query):
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ªæŸ¥è¯¢å’Œæ€»ç»“çš„å­ä»»åŠ¡"""
        try:
            # 1. å¼‚æ­¥æœç´¢
            search_results = await search_tool.ainvoke(query)

            # å‡†å¤‡ä¸Šä¸‹æ–‡å’Œå¼•ç”¨æ˜ å°„
            context_blocks = []
            citation_map = {}
            for i, res in enumerate(search_results):
                # ä¸ºæ¯ä¸ªæ¥æºåˆ†é…ä¸€ä¸ªä¸´æ—¶ç¼–å·ç”¨äºæ‘˜è¦å¼•ç”¨
                citation_map[f"[{i + 1}]"] = res['url']
                context_blocks.append(f"ã€æ¥æº {i + 1}ã€‘: {res['content']}")

            context = "\n".join(context_blocks)

            # 2. å¼‚æ­¥æ€»ç»“ (Information Extraction)
            summary_prompt = (
                f"é’ˆå¯¹æŸ¥è¯¢ '{query}'ï¼Œä»ä»¥ä¸‹ã€æ¥æºã€‘ä¸­æå–å…³é”®äº‹å®ã€æ•°æ®å’Œè§‚ç‚¹ã€‚"
                "åœ¨æ‘˜è¦ä¸­ï¼ŒåŠ¡å¿…ä½¿ç”¨æ ¼å¼ã€æ¥æº Xã€‘å¼•ç”¨ä½ ä½¿ç”¨çš„ä»»ä½•ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š'äººå·¥æ™ºèƒ½æŠ•èµ„åœ¨2023å¹´å¢é•¿äº†30%ã€æ¥æº 2ã€‘'ã€‚"
                "å¿½ç•¥æ— å…³ä¿¡æ¯ã€‚ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ï¼Œå¹¶åˆ—å‡ºå®Œæ•´çš„å¼•ç”¨æ˜ å°„ã€‚"
                f"æœ€ç»ˆè¿”å›æ ¼å¼ï¼š\n---\næ‘˜è¦å†…å®¹\n---\nå¼•ç”¨: {citation_map}"
            )

            summary_response = await llm.ainvoke([
                SystemMessage(content=summary_prompt),
                HumanMessage(content=context)
            ])

            # æå–æ‘˜è¦å’Œå¼•ç”¨ï¼Œå¹¶å°†å…¶åˆå¹¶æˆä¸€ä¸ª V4 æ ¼å¼çš„å‘ç°å—
            return f"### å…³äº '{query}' çš„å‘ç° (ç¬¬ {loop_idx} è½®):\n{summary_response.content}"

        except Exception as e:
            print(f"  âŒ æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
            return None

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
    tasks = [process_query(query) for query in queries]
    results = await asyncio.gather(*tasks)

    new_findings = [r for r in results if r]
    total_findings = state["all_findings"] + new_findings

    return {"all_findings": total_findings, "loop_count": loop_idx}


async def evaluate_findings(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 3ï¼šè¯„ä¼°ä¸åæ€ã€‘
    æŸ¥çœ‹å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå†™æŠ¥å‘Šã€‚
    """
    print("\nğŸ¤” [è¯„ä¼°] æ­£åœ¨æ£€æŸ¥èµ„æ–™å®Œæ•´æ€§...")

    topic = state["topic"]
    findings_text = "\n\n".join(state["all_findings"])
    loop_count = state["loop_count"]

    if loop_count >= MAX_LOOPS:
        print("ğŸ›‘ [è¯„ä¼°] å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢æœç´¢ã€‚")
        return {"missing_info": "sufficient"}

        # è®© LLM è¯„ä¼°
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªè‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆã€‚"
        "è¯·é˜…è¯»ç›®å‰æ”¶é›†åˆ°çš„ç¬”è®°ï¼Œåˆ¤æ–­æ˜¯å¦è¶³ä»¥æ’°å†™å…³äºè¯¥ä¸»é¢˜çš„æ·±åº¦æŠ¥å‘Šã€‚"
        "å¦‚æœèµ„æ–™å……è¶³ï¼Œè¯·åªå›å¤ 'SUFFICIENT'ã€‚"
        "å¦‚æœèµ„æ–™ç¼ºå¤±ï¼ˆä¾‹å¦‚ç¼ºå°‘å…·ä½“æ•°æ®ã€åé¢è§‚ç‚¹ã€æœ€æ–°è¿›å±•ï¼‰ï¼Œè¯·å›å¤ 'MISSING: <ç¼ºå¤±å†…å®¹çš„æè¿°>'ã€‚"
        "ä¸è¦å®¢æ°”ï¼Œå¦‚æœä¿¡æ¯å¤ªæµ…æ˜¾ï¼Œå¿…é¡»è¦æ±‚ç»§ç»­æ·±æŒ–ã€‚"
    )

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç ”ç©¶ä¸»é¢˜: {topic}\n\nç›®å‰ç¬”è®°:\n{findings_text}")
    ])

    if "SUFFICIENT" in response.content.upper():
        print("âœ… [è¯„ä¼°] èµ„æ–™å·²å……è¶³ï¼")
        return {"missing_info": "sufficient"}
    else:
        print(f"âš ï¸ [è¯„ä¼°] å‘ç°ç¼ºå£: {response.content}")
        return {"missing_info": response.content}


async def generate_new_queries(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 4ï¼šç”Ÿæˆè¡¥å……æŸ¥è¯¢ã€‘
    å¦‚æœ evaluate è®¤ä¸ºä¿¡æ¯ç¼ºå¤±ï¼Œè¿™é‡Œè´Ÿè´£ç”Ÿæˆé’ˆå¯¹æ€§çš„æ–°æŸ¥è¯¢ã€‚
    """
    missing_info = state["missing_info"]
    print("\nğŸ”„ [è¿­ä»£] æ­£åœ¨ç”Ÿæˆè¡¥å……æŸ¥è¯¢ä»¥å¡«è¡¥ç¼ºå£...")

    system_prompt = (
        "æ ¹æ®ç¼ºå¤±çš„ä¿¡æ¯æè¿°ï¼Œç”Ÿæˆ 2 ä¸ªå…·ä½“çš„æœç´¢å¼•æ“æŸ¥è¯¢è¯­å¥æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç¼ºå¤±ä¿¡æ¯: {missing_info}")
    ])

    new_queries = [line.strip() for line in response.content.split('\n') if line.strip()][:2]
    print(f"ğŸ†• [è¡¥å……æŸ¥è¯¢] {new_queries}")

    return {"current_queries": new_queries}


async def outline_report(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 5ï¼šåŠ¨æ€ç”ŸæˆæŠ¥å‘Šå¤§çº²ã€‘ - æ ¹æ®ä¸»é¢˜ç±»åˆ«å’Œåˆæ­¥å‘ç°ç”Ÿæˆå¤§çº²ã€‚
    """
    print("\nğŸ“ [ç»“æ„] æ­£åœ¨ç”ŸæˆåŠ¨æ€æŠ¥å‘Šå¤§çº²...")

    topic = state["topic"]
    category = state["topic_category"]
    findings_preview = "\n\n".join(state["all_findings"])[:2000]  # ä¼ é€’éƒ¨åˆ†å‘ç°ä½œä¸ºä¸Šä¸‹æ–‡

    system_prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªé«˜çº§æŠ¥å‘Šç»“æ„å¸ˆã€‚ä¸»é¢˜ç±»åˆ«æ˜¯ '{category}'ã€‚"
        "è¯·æ ¹æ®è¿™ä¸ªç±»åˆ«å’Œä»¥ä¸‹åˆæ­¥ç ”ç©¶ç¬”è®°ï¼Œç”Ÿæˆä¸€ä»½æœ€ä¸“ä¸šã€æœ€ç›¸å…³çš„æŠ¥å‘Šå¤§çº²ã€‚"
        "å¤§çº²åº”è‡³å°‘åŒ…å« 4 ä¸ªä¸»è¦ç« èŠ‚ï¼ˆMarkdown äºŒçº§æ ‡é¢˜ ##ï¼‰ï¼Œå¹¶ç›´æ¥è¿”å› Markdown æ ¼å¼çš„å¤§çº²ã€‚"
    )

    user_prompt = f"ç ”ç©¶ä¸»é¢˜: {topic}\nä¸»é¢˜ç±»åˆ«: {category}\n\nåˆæ­¥ç ”ç©¶ç¬”è®°é¢„è§ˆ:\n{findings_preview}"

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    print(f"âœ… [ç»“æ„] å¤§çº²å·²ç”Ÿæˆï¼ŒåŸºäºç±»åˆ«: {category}ã€‚")
    return {"report_outline": response.content}


async def context_refiner(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 6 (V5 æ–°å¢)ï¼šä¸Šä¸‹æ–‡ç²¾ç‚¼å™¨ã€‘
    æ ¹æ®å¤§çº²å’Œæ‰€æœ‰èµ„æ–™ï¼Œæç‚¼å‡ºæœ€å…³é”®çš„ä¸Šä¸‹æ–‡ï¼Œç”¨äºæŒ‡å¯¼å†™ä½œã€‚
    åœ¨ä¼ä¸šçº§åº”ç”¨ä¸­ï¼Œæ­¤èŠ‚ç‚¹ä¼šæ‰§è¡Œ RAG æ£€ç´¢ã€‚
    """
    print("\nâœ‚ï¸ [ç²¾ç‚¼] æ­£åœ¨æ ¹æ®å¤§çº²ï¼Œä»å…¨éƒ¨èµ„æ–™ä¸­æç‚¼æ ¸å¿ƒä¸Šä¸‹æ–‡...")

    outline = state["report_outline"]
    all_findings = "\n\n".join(state["all_findings"])

    # è¿™ä¸€æ­¥æ—¨åœ¨æ¨¡æ‹ŸRAGä¸­çš„â€œè¿‡æ»¤å’Œæ’åºâ€ï¼Œåªä¿ç•™ä¸å¤§çº²å¼ºç›¸å…³çš„éƒ¨åˆ†ã€‚
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å‹ç¼©ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„æŠ¥å‘Šå¤§çº²ï¼Œä»ä¸‹æ–¹æ‰€æœ‰ç ”ç©¶å‘ç°ä¸­ï¼Œ"
        "ä»…æŒ‘é€‰å‡º**ä¸å¤§çº²æ¯ä¸ªç« èŠ‚ç›´æ¥ç›¸å…³**çš„äº‹å®ã€æ•°æ®å’Œå¼•ç”¨ä¿¡æ¯ã€‚"
        "è¯·å°†æç‚¼åçš„ä¿¡æ¯ä»¥ç»“æ„åŒ–ï¼ˆæŒ‰å¤§çº²ç« èŠ‚ç»„ç»‡ï¼‰çš„æ–¹å¼è¿”å›ï¼Œ**åŠ¡å¿…ä¿ç•™æ‰€æœ‰ã€æ¥æº Xã€‘æ ‡è®°**ã€‚"
        "ç›®æ ‡ï¼šå°†ä¸Šä¸‹æ–‡å‹ç¼©åˆ°æœ€ç²¾ç®€ï¼Œä½†ä¿ç•™æ‰€æœ‰æ ¸å¿ƒè®ºæ®ã€‚"
    )

    user_prompt = f"æŠ¥å‘Šå¤§çº²:\n{outline}\n\nå…¨éƒ¨åŸå§‹ç ”ç©¶å‘ç°:\n{all_findings}"

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    print("âœ¨ [ç²¾ç‚¼] æ ¸å¿ƒä¸Šä¸‹æ–‡å·²æç‚¼å®Œæˆã€‚")
    return {"refined_context": response.content}


async def write_report(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 7ï¼šæ’°å†™æŠ¥å‘Šã€‘ - V5: ä½¿ç”¨ç²¾ç‚¼åçš„ä¸Šä¸‹æ–‡è¿›è¡Œå†™ä½œã€‚
    """
    print("\nâœï¸ [å†™ä½œ] æ­£åœ¨æ•´åˆç²¾ç‚¼åçš„èµ„æ–™æ’°å†™æŠ¥å‘Š...")

    # V5 æ ¸å¿ƒå˜åŒ–ï¼šä½¿ç”¨ refined_contextï¼Œè€Œä¸æ˜¯åŸå§‹çš„ all_findings
    refined_context = state["refined_context"]
    outline = state["report_outline"]

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„**ç²¾ç‚¼ä¸Šä¸‹æ–‡**å’Œä»¥ä¸‹å¤§çº²ï¼Œå†™å‡ºä¸€ä»½ç»“æ„ä¸¥è°¨ã€æ•°æ®è¯¦å®çš„æ·±åº¦æŠ¥å‘Š(Markdownæ ¼å¼)ã€‚"
        "ä¸¥æ ¼éµå¾ªå¤§çº²ç»“æ„ã€‚"
        "å†™ä½œæ—¶ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ã€æ¥æº Xã€‘æ ‡è®°ï¼Œå¹¶åœ¨æŠ¥å‘Šæœ«å°¾çš„'å‚è€ƒèµ„æ–™'éƒ¨åˆ†å®Œæ•´åˆ—å‡ºæ‰€æœ‰å¼•ç”¨çš„ URLã€‚"
        "è¯·å…ˆå†™æ­£æ–‡ï¼Œå†åœ¨æŠ¥å‘Šæœ«å°¾æ·»åŠ å‚è€ƒèµ„æ–™éƒ¨åˆ†ã€‚"
    )

    user_prompt = f"ä¸»é¢˜: {state['topic']}\n\nç»“æ„å¤§çº²:\n{outline}\n\nç²¾ç‚¼ä¸Šä¸‹æ–‡:\n{refined_context}"

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    return {"final_report": response.content}


# ==============================================================================
# 5. æ„å»ºå›¾é€»è¾‘ (Routing Logic)
# ==============================================================================

def should_continue(state: ResearchState):
    """
    æ¡ä»¶è¾¹é€»è¾‘ï¼šå†³å®šæ˜¯å›å»æ¥ç€æœï¼Œè¿˜æ˜¯å»å†™æŠ¥å‘Š
    """
    missing = state.get("missing_info", "")
    if missing == "sufficient" or state["loop_count"] >= MAX_LOOPS:
        return "to_outline"
    else:
        return "to_generator"


# åˆå§‹åŒ–å›¾
workflow = StateGraph(ResearchState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("planner", plan_research)
workflow.add_node("researcher", execute_search)
workflow.add_node("evaluator", evaluate_findings)
workflow.add_node("query_generator", generate_new_queries)
workflow.add_node("outline_planner", outline_report)
workflow.add_node("context_refiner", context_refiner)  # [V5 æ–°å¢èŠ‚ç‚¹]
workflow.add_node("writer", write_report)

# æ„å»ºæµç¨‹
workflow.set_entry_point("planner")
workflow.add_edge("planner", "researcher")
workflow.add_edge("researcher", "evaluator")

# è¯„ä¼° -> æ¡ä»¶åˆ¤æ–­
workflow.add_conditional_edges(
    "evaluator",
    should_continue,
    {
        "to_generator": "query_generator",
        "to_outline": "outline_planner"
    }
)

# è¿­ä»£å¾ªç¯
workflow.add_edge("query_generator", "researcher")

# ç»“æ„åŒ–å†™ä½œ (V5 è·¯å¾„: å¤§çº² -> ç²¾ç‚¼ -> å†™ä½œ)
workflow.add_edge("outline_planner", "context_refiner")  # [V5 æ›´æ”¹]
workflow.add_edge("context_refiner", "writer")  # [V5 æ›´æ”¹]
workflow.add_edge("writer", END)

app = workflow.compile()


# ==============================================================================
# 6. è¿è¡Œå…¥å£
# ==============================================================================

async def run_agent():
    print("=== Deep Research Agent V5 (ä¸Šä¸‹æ–‡ç²¾ç‚¼ & æ‰©å±•å°±ç»ª) ===")
    topic = input("è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜: ")
    if not topic: topic = "é‡å­è®¡ç®—æœºåœ¨2024å¹´çš„æœ€æ–°çªç ´"

    initial_state = {"topic": topic}

    final_state = await app.ainvoke(initial_state)

    print("\n" + "=" * 50)
    print("æœ€ç»ˆæŠ¥å‘Š:")
    print(final_state["final_report"])

    # ä¿å­˜æ–‡ä»¶
    with open("deep_research_v5.md", "w", encoding="utf-8") as f:
        f.write(final_state["final_report"])
    print("\n[ç³»ç»Ÿ] æŠ¥å‘Šå·²ä¿å­˜è‡³ deep_research_v5.md")


if __name__ == "__main__":
    asyncio.run(run_agent())
