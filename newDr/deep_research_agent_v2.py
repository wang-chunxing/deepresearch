import os
import operator
from typing import Annotated, List, TypedDict, Union

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# å¼•å…¥ LangChain å’Œ LangGraph ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END

# ==============================================================================
# 1. é…ç½® API Key (ä¿æŒä¸ V1 ä¸€è‡´)
# ==============================================================================
api_key = os.environ.get("ARK_API_KEY") or os.environ.get("DOUBAO_API_KEY")
tavily_key = os.environ.get("TAVILY_API_KEY")

if not api_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° ARK/DOUBAO API Keyï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ LLMã€‚")

if not tavily_key:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚")


# ==============================================================================
# 2. å®šä¹‰çŠ¶æ€ (State) - V2 å‡çº§ç‰ˆ
# ==============================================================================
class ResearchState(TypedDict):
    topic: str  # åŸå§‹ç ”ç©¶ä¸»é¢˜
    current_queries: List[str]  # å½“å‰è¿™ä¸€è½®éœ€è¦æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢
    all_findings: List[str]  # ç´¯ç§¯æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ (V2 æ”¯æŒå¤šè½®ç´¯ç§¯)
    loop_count: int  # å½“å‰è¿­ä»£æ¬¡æ•° (é˜²æ­¢æ­»å¾ªç¯)
    missing_info: str  # è¯„ä¼°é˜¶æ®µå‘ç°çš„ç¼ºå¤±ä¿¡æ¯ (ç”¨äºæŒ‡å¯¼ä¸‹ä¸€è½®)
    final_report: str  # æœ€ç»ˆæŠ¥å‘Š


# ==============================================================================
# 3. åˆå§‹åŒ–æ¨¡å‹å’Œå·¥å…·
# ==============================================================================
# è±†åŒ…æ¨¡å‹
class _LLMResponse:
    def __init__(self, content: str):
        self.content = content

class _DummyLLM:
    def invoke(self, messages):
        sys = messages[0].content if messages else ""
        human = messages[-1].content if messages else ""
        if "æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢" in sys or "æŸ¥è¯¢åˆ—è¡¨" in sys:
            topic = human.split("ä¸»é¢˜:")[-1].strip() if "ä¸»é¢˜:" in human else "ä¸»é¢˜"
            return _LLMResponse(f"{topic} å®šä¹‰\n{topic} ç°çŠ¶\n{topic} äº‰è®®")
        if "æå–å…³é”®äº‹å®" in sys:
            return _LLMResponse("è¦ç‚¹1ã€æ¥æº 1ã€‘\nè¦ç‚¹2ã€æ¥æº 2ã€‘\nå¼•ç”¨: {'[1]': 'https://example.com/1', '[2]': 'https://example.com/2'}")
        if "è‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆ" in sys:
            return _LLMResponse("SUFFICIENT")
        if "ä¸“ä¸šåˆ†æå¸ˆ" in sys:
            return _LLMResponse("# æ·±åº¦æŠ¥å‘Š\n\n- æ ¸å¿ƒå‘ç°\n- åˆ†æ\n- ç»“è®º")
        return _LLMResponse("ç¤ºä¾‹è¾“å‡º")

class _DummySearch:
    def __init__(self, max_results: int = 3):
        self.max_results = max_results
    def invoke(self, query: str):
        return [{"url": f"https://example.com/{i}", "content": f"ä¸{query}ç›¸å…³çš„ç¤ºä¾‹å†…å®¹ {i}"} for i in range(1, self.max_results + 1)]

llm = ChatOpenAI(
    model="doubao-seed-1-6-251015",
    api_key=api_key,
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    temperature=0.1,
) if api_key else _DummyLLM()

search_tool = TavilySearchResults(max_results=3, tavily_api_key=tavily_key) if tavily_key else _DummySearch(max_results=3)

# æœ€å¤§è¿­ä»£æ¬¡æ•° (é˜²æ­¢ä¸€ç›´æœä¸ªæ²¡å®Œ)
MAX_LOOPS = 3


# ==============================================================================
# 4. å®šä¹‰èŠ‚ç‚¹é€»è¾‘ (Nodes)
# ==============================================================================

def plan_research(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 1ï¼šåˆå§‹è§„åˆ’ã€‘
    ä¸ V1 ç±»ä¼¼ï¼Œç”Ÿæˆåˆå§‹çš„ä¸€ç»„æŸ¥è¯¢ã€‚
    """
    print(f"\nğŸš€ [å¯åŠ¨] å¼€å§‹ç ”ç©¶ä¸»é¢˜: {state['topic']}")

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„ä¸»é¢˜æ‹†è§£ä¸º 3 ä¸ªåˆå§‹æœç´¢æŸ¥è¯¢ã€‚"
        "æŸ¥è¯¢åº”æ¶µç›–åŸºç¡€å®šä¹‰ã€ç°çŠ¶å’Œä¸»è¦äº‰è®®ç‚¹ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}")
    ])

    queries = [line.strip() for line in response.content.split('\n') if line.strip()][:3]
    print(f"ğŸ“‹ [è§„åˆ’] åˆå§‹æŸ¥è¯¢: {queries}")

    # åˆå§‹åŒ–çŠ¶æ€
    return {
        "current_queries": queries,
        "all_findings": [],
        "loop_count": 0,
        "missing_info": ""
    }


def execute_search(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 2ï¼šæ‰§è¡Œæœç´¢ã€‘
    æ‰§è¡Œ current_queries ä¸­çš„æŸ¥è¯¢ï¼Œå¹¶å°†ç»“æœè¿½åŠ åˆ° all_findings ä¸­ã€‚
    """
    loop_idx = state["loop_count"] + 1
    print(f"\nğŸ” [ç¬¬ {loop_idx} è½®æœç´¢] æ­£åœ¨æ‰§è¡Œ {len(state['current_queries'])} ä¸ªæŸ¥è¯¢...")

    new_findings = []

    for query in state["current_queries"]:
        try:
            # æœç´¢
            search_results = search_tool.invoke(query)
            # ç¡®ä¿search_resultsæ˜¯åˆ—è¡¨å¹¶ä¸”æ­£ç¡®å¤„ç†å…¶å†…å®¹
            if isinstance(search_results, list):
                # å®‰å…¨åœ°å¤„ç†æœç´¢ç»“æœï¼Œé˜²æ­¢ç´¢å¼•é”™è¯¯
                processed_results = []
                for res in search_results:
                    if isinstance(res, dict) and 'content' in res and 'url' in res:
                        processed_results.append(f"- {res['content']} (æ¥æº: {res['url']})")
                context = "\n".join(processed_results)
            else:
                # å¤„ç†éé¢„æœŸçš„è¿”å›æ ¼å¼
                context = str(search_results)

            # æ€»ç»“ (Information Extraction)
            summary_prompt = (
                f"é’ˆå¯¹æŸ¥è¯¢ '{query}'ï¼Œä»ä»¥ä¸‹æœç´¢ç»“æœä¸­æå–å…³é”®äº‹å®ã€æ•°æ®å’Œè§‚ç‚¹ã€‚"
                "å¿½ç•¥æ— å…³ä¿¡æ¯ã€‚ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ã€‚"
            )
            summary = llm.invoke([
                SystemMessage(content=summary_prompt),
                HumanMessage(content=context)
            ]).content

            new_findings.append(f"ã€ç¬¬ {loop_idx} è½® - {query}ã€‘:\n{summary}")

        except Exception as e:
            print(f"  âŒ æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")

    # å°†æ–°å‘ç°è¿½åŠ åˆ°ç°æœ‰çš„å‘ç°åˆ—è¡¨ä¸­ (ä½¿ç”¨ operator.add é€»è¾‘æˆ–ç›´æ¥åˆ—è¡¨ç›¸åŠ )
    # åœ¨ LangGraph ä¸­ï¼Œå¦‚æœæˆ‘ä»¬è¿”å› key çš„å€¼ï¼Œé»˜è®¤æ˜¯è¦†ç›–ã€‚
    # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨åˆå¹¶åˆ—è¡¨è¿”å›ã€‚
    total_findings = state["all_findings"] + new_findings

    return {"all_findings": total_findings, "loop_count": loop_idx}


def evaluate_findings(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 3 (V2æ–°å¢)ï¼šè¯„ä¼°ä¸åæ€ã€‘
    æŸ¥çœ‹å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå†™æŠ¥å‘Šã€‚
    å¦‚æœä¸å¤Ÿï¼Œç”Ÿæˆæ–°çš„æŸ¥è¯¢æ¥å¡«è¡¥ç©ºç™½ã€‚
    """
    print("\nğŸ¤” [è¯„ä¼°] æ­£åœ¨æ£€æŸ¥èµ„æ–™å®Œæ•´æ€§...")

    topic = state["topic"]
    findings_text = "\n\n".join(state["all_findings"])
    loop_count = state["loop_count"]

    # å¦‚æœè¾¾åˆ°æœ€å¤§æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ
    if loop_count >= MAX_LOOPS:
        print("ğŸ›‘ [è¯„ä¼°] å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢æœç´¢ã€‚")
        return {"missing_info": "sufficient"}  # æ ‡è®°ä¸ºè¶³å¤Ÿï¼Œè¿«ä½¿è¿›å…¥å†™ä½œ

    # è®© LLM è¯„ä¼°
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªè‹›åˆ»çš„ç ”ç©¶å¯¼å¸ˆã€‚"
        "è¯·é˜…è¯»ç›®å‰æ”¶é›†åˆ°çš„ç¬”è®°ï¼Œåˆ¤æ–­æ˜¯å¦è¶³ä»¥æ’°å†™å…³äºè¯¥ä¸»é¢˜çš„æ·±åº¦æŠ¥å‘Šã€‚"
        "å¦‚æœèµ„æ–™å……è¶³ï¼Œè¯·åªå›å¤ 'SUFFICIENT'ã€‚"
        "å¦‚æœèµ„æ–™ç¼ºå¤±ï¼ˆä¾‹å¦‚ç¼ºå°‘å…·ä½“æ•°æ®ã€åé¢è§‚ç‚¹ã€æœ€æ–°è¿›å±•ï¼‰ï¼Œè¯·å›å¤ 'MISSING: <ç¼ºå¤±å†…å®¹çš„æè¿°>'ã€‚"
        "ä¸è¦å®¢æ°”ï¼Œå¦‚æœä¿¡æ¯å¤ªæµ…æ˜¾ï¼Œå¿…é¡»è¦æ±‚ç»§ç»­æ·±æŒ–ã€‚"
    )

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç ”ç©¶ä¸»é¢˜: {topic}\n\nç›®å‰ç¬”è®°:\n{findings_text}")
    ]).content

    if "SUFFICIENT" in response.upper():
        print("âœ… [è¯„ä¼°] èµ„æ–™å·²å……è¶³ï¼")
        return {"missing_info": "sufficient"}
    else:
        print(f"âš ï¸ [è¯„ä¼°] å‘ç°ç¼ºå£: {response}")
        return {"missing_info": response}


def generate_new_queries(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 4 (V2æ–°å¢)ï¼šç”Ÿæˆè¡¥å……æŸ¥è¯¢ã€‘
    å¦‚æœ evaluate è®¤ä¸ºä¿¡æ¯ç¼ºå¤±ï¼Œè¿™é‡Œè´Ÿè´£ç”Ÿæˆé’ˆå¯¹æ€§çš„æ–°æŸ¥è¯¢ã€‚
    """
    missing_info = state["missing_info"]
    print("\nğŸ”„ [è¿­ä»£] æ­£åœ¨ç”Ÿæˆè¡¥å……æŸ¥è¯¢ä»¥å¡«è¡¥ç¼ºå£...")

    system_prompt = (
        "æ ¹æ®ç¼ºå¤±çš„ä¿¡æ¯æè¿°ï¼Œç”Ÿæˆ 2 ä¸ªå…·ä½“çš„æœç´¢å¼•æ“æŸ¥è¯¢è¯­å¥æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚"
        "åªè¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    )

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç¼ºå¤±ä¿¡æ¯: {missing_info}")
    ])

    new_queries = [line.strip() for line in response.content.split('\n') if line.strip()][:2]
    print(f"ğŸ†• [è¡¥å……æŸ¥è¯¢] {new_queries}")

    return {"current_queries": new_queries}


def write_report(state: ResearchState):
    """
    ã€èŠ‚ç‚¹ 5ï¼šæ’°å†™æŠ¥å‘Šã€‘
    """
    print("\nâœï¸ [å†™ä½œ] æ­£åœ¨æ•´åˆæ‰€æœ‰èµ„æ–™æ’°å†™æŠ¥å‘Š...")

    context = "\n\n".join(state["all_findings"])
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„æµ·é‡ç ”ç©¶ç¬”è®°ï¼Œå†™å‡ºä¸€ä»½ç»“æ„ä¸¥è°¨ã€æ•°æ®è¯¦å®çš„æ·±åº¦æŠ¥å‘Š(Markdownæ ¼å¼)ã€‚"

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ä¸»é¢˜: {state['topic']}\n\nç¬”è®°:\n{context}")
    ])

    return {"final_report": response.content}


# ==============================================================================
# 5. æ„å»ºå›¾é€»è¾‘ (Routing Logic)
# ==============================================================================

def should_continue(state: ResearchState):
    """
    æ¡ä»¶è¾¹é€»è¾‘ï¼šå†³å®šæ˜¯å›å»æ¥ç€æœï¼Œè¿˜æ˜¯å»å†™æŠ¥å‘Š
    """
    missing = state.get("missing_info", "")
    if missing == "sufficient" or state["loop_count"] >= MAX_LOOPS:
        return "to_writer"
    else:
        return "to_generator"


# åˆå§‹åŒ–å›¾
workflow = StateGraph[ResearchState, None, ResearchState, ResearchState](ResearchState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("planner", plan_research)
workflow.add_node("researcher", execute_search)
workflow.add_node("evaluator", evaluate_findings)
workflow.add_node("query_generator", generate_new_queries)
workflow.add_node("writer", write_report)

# æ„å»ºæµç¨‹
# 1. å¼€å§‹ -> è§„åˆ’
workflow.set_entry_point("planner")
# 2. è§„åˆ’ -> æœç´¢
workflow.add_edge("planner", "researcher")
# 3. æœç´¢ -> è¯„ä¼°
workflow.add_edge("researcher", "evaluator")

# 4. è¯„ä¼° -> æ¡ä»¶åˆ¤æ–­ (ç»§ç»­æœ è¿˜æ˜¯ å†™æŠ¥å‘Š?)
workflow.add_conditional_edges(
    "evaluator",
    should_continue,
    {
        "to_generator": "query_generator",  # ç¼ºä¿¡æ¯ -> ç”Ÿæˆæ–°æŸ¥è¯¢
        "to_writer": "writer"  # å¤Ÿäº† -> å†™æŠ¥å‘Š
    }
)

# 5. ç”Ÿæˆæ–°æŸ¥è¯¢ -> å›åˆ°æœç´¢ (é—­ç¯)
workflow.add_edge("query_generator", "researcher")

# 6. å†™æŠ¥å‘Š -> ç»“æŸ
workflow.add_edge("writer", END)

app = workflow.compile()

# ==============================================================================
# 6. è¿è¡Œå…¥å£
# ==============================================================================
if __name__ == "__main__":
    print("=== Deep Research Agent V2 (Self-Correcting) ===")
    topic = input("è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜: ")
    if not topic: topic = "é‡å­è®¡ç®—æœºåœ¨2024å¹´çš„æœ€æ–°çªç ´"

    initial_state = {"topic": topic}

    # è¿è¡Œå›¾
    final_state = app.invoke(initial_state)

    print("\n" + "=" * 50)
    print(final_state["final_report"])

    # ä¿å­˜æ–‡ä»¶
    with open("deep_research_v2.md", "w", encoding="utf-8") as f:
        f.write(final_state["final_report"])
